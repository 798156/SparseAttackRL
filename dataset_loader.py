# dataset_loader.py
"""
å¤šæ•°æ®é›†åŠ è½½å™¨
æ”¯æŒï¼šCIFAR-10, CIFAR-100, ImageNet (å­é›†)
"""

import torch
from torchvision import datasets, transforms
import os


class DatasetLoader:
    """ç»Ÿä¸€çš„æ•°æ®é›†åŠ è½½æ¥å£"""
    
    def __init__(self, dataset_name='cifar10', data_root='./data'):
        """
        å‚æ•°:
            dataset_name: 'cifar10', 'cifar100', 'imagenet'
            data_root: æ•°æ®å­˜å‚¨è·¯å¾„
        """
        self.dataset_name = dataset_name.lower()
        self.data_root = data_root
        
        # æ ¹æ®æ•°æ®é›†è®¾ç½®å‚æ•°
        if self.dataset_name == 'cifar10':
            self.num_classes = 10
            self.image_size = 32
            self.mean = [0.4914, 0.4822, 0.4465]
            self.std = [0.2023, 0.1994, 0.2010]
        elif self.dataset_name == 'cifar100':
            self.num_classes = 100
            self.image_size = 32
            self.mean = [0.5071, 0.4867, 0.4408]
            self.std = [0.2675, 0.2565, 0.2761]
        elif self.dataset_name == 'imagenet':
            self.num_classes = 1000
            self.image_size = 224
            self.mean = [0.485, 0.456, 0.406]
            self.std = [0.229, 0.224, 0.225]
        else:
            raise ValueError(f"Unsupported dataset: {dataset_name}")
    
    def get_transform(self, train=False):
        """è·å–æ•°æ®å˜æ¢"""
        if train:
            # è®­ç»ƒæ—¶å¯ä»¥æ·»åŠ æ•°æ®å¢å¼º
            transform_list = [
                transforms.ToTensor(),
                transforms.Normalize(self.mean, self.std)
            ]
        else:
            # æµ‹è¯•æ—¶åªåšå½’ä¸€åŒ–
            if self.dataset_name == 'imagenet':
                transform_list = [
                    transforms.Resize(256),
                    transforms.CenterCrop(224),
                    transforms.ToTensor(),
                    transforms.Normalize(self.mean, self.std)
                ]
            else:
                transform_list = [
                    transforms.ToTensor(),
                    transforms.Normalize(self.mean, self.std)
                ]
        
        return transforms.Compose(transform_list)
    
    def load_test_set(self):
        """åŠ è½½æµ‹è¯•é›†"""
        transform = self.get_transform(train=False)
        
        if self.dataset_name == 'cifar10':
            test_set = datasets.CIFAR10(
                root=self.data_root,
                train=False,
                download=True,
                transform=transform
            )
        
        elif self.dataset_name == 'cifar100':
            test_set = datasets.CIFAR100(
                root=self.data_root,
                train=False,
                download=True,
                transform=transform
            )
        
        elif self.dataset_name == 'imagenet':
            # ImageNetéœ€è¦æ‰‹åŠ¨ä¸‹è½½
            # è¿™é‡Œä½¿ç”¨éªŒè¯é›†çš„ä¸€ä¸ªå­é›†
            imagenet_path = os.path.join(self.data_root, 'imagenet', 'val')
            if not os.path.exists(imagenet_path):
                print(f"âš ï¸ ImageNet path not found: {imagenet_path}")
                print("   è¯·æ‰‹åŠ¨ä¸‹è½½ImageNetéªŒè¯é›†å¹¶è§£å‹åˆ°è¯¥è·¯å¾„")
                print("   æˆ–è€…è·³è¿‡ImageNetå®éªŒ")
                return None
            
            test_set = datasets.ImageFolder(
                root=imagenet_path,
                transform=transform
            )
        
        else:
            raise ValueError(f"Unsupported dataset: {self.dataset_name}")
        
        return test_set
    
    def get_sample_subset(self, test_set, num_samples=500, seed=42):
        """
        ä»æµ‹è¯•é›†ä¸­éšæœºé‡‡æ ·å­é›†
        
        å‚æ•°:
            test_set: æµ‹è¯•é›†
            num_samples: é‡‡æ ·æ•°é‡
            seed: éšæœºç§å­
        
        è¿”å›:
            indices: é‡‡æ ·çš„ç´¢å¼•åˆ—è¡¨
        """
        import numpy as np
        np.random.seed(seed)
        
        total_samples = len(test_set)
        num_samples = min(num_samples, total_samples)
        
        # éšæœºé‡‡æ ·
        indices = np.random.choice(total_samples, num_samples, replace=False)
        
        return indices.tolist()
    
    def get_dataset_info(self):
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        return {
            'name': self.dataset_name,
            'num_classes': self.num_classes,
            'image_size': self.image_size,
            'mean': self.mean,
            'std': self.std
        }


def get_all_datasets(data_root='./data'):
    """
    è·å–æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†
    
    è¿”å›:
        datasets_dict: {dataset_name: DatasetLoader}
    """
    datasets_dict = {}
    
    # CIFAR-10ï¼ˆæ€»æ˜¯å¯ç”¨ï¼‰
    datasets_dict['cifar10'] = DatasetLoader('cifar10', data_root)
    
    # CIFAR-100ï¼ˆæ€»æ˜¯å¯ç”¨ï¼‰
    datasets_dict['cifar100'] = DatasetLoader('cifar100', data_root)
    
    # ImageNetï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    imagenet_loader = DatasetLoader('imagenet', data_root)
    imagenet_path = os.path.join(data_root, 'imagenet', 'val')
    if os.path.exists(imagenet_path):
        datasets_dict['imagenet'] = imagenet_loader
    else:
        print("âš ï¸ ImageNet not found, skipping ImageNet experiments")
    
    return datasets_dict


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•æ•°æ®é›†åŠ è½½å™¨")
    
    # æµ‹è¯•CIFAR-10
    loader = DatasetLoader('cifar10')
    print(f"\næ•°æ®é›†: {loader.get_dataset_info()}")
    
    test_set = loader.load_test_set()
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_set)}")
    
    # é‡‡æ ·å­é›†
    indices = loader.get_sample_subset(test_set, num_samples=100)
    print(f"é‡‡æ ·ç´¢å¼•æ•°é‡: {len(indices)}")
    
    # æµ‹è¯•CIFAR-100
    loader100 = DatasetLoader('cifar100')
    print(f"\næ•°æ®é›†: {loader100.get_dataset_info()}")
    test_set100 = loader100.load_test_set()
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_set100)}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")

