# test_grad_modification.py
"""æµ‹è¯•åœ¨requires_grad=Trueçš„tensorä¸Šä½¿ç”¨no_gradä¿®æ”¹çš„è¡Œä¸º"""

import torch

print("=" * 70)
print("ğŸ§ª æµ‹è¯•ä¿®æ”¹requires_grad=Trueçš„tensor")
print("=" * 70)

# æµ‹è¯•1: æ­£å¸¸ä¿®æ”¹
print("\næµ‹è¯•1: ä¸ä½¿ç”¨no_gradä¿®æ”¹")
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
print(f"  ä¿®æ”¹å‰: {x}")
x_copy = x.clone()
x_copy[0] += 10.0
print(f"  ä¿®æ”¹åclone: {x_copy}")
print(f"  åŸå§‹x: {x}")

# æµ‹è¯•2: ä½¿ç”¨no_gradä¿®æ”¹
print("\næµ‹è¯•2: ä½¿ç”¨no_gradä¿®æ”¹requires_grad=Trueçš„tensor")
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
print(f"  ä¿®æ”¹å‰: {x}")
with torch.no_grad():
    x[0] += 10.0
print(f"  ä¿®æ”¹å: {x}")
print(f"  æˆåŠŸä¿®æ”¹!")

# æµ‹è¯•3: å…ˆdetachå†ä¿®æ”¹
print("\næµ‹è¯•3: ä¿®æ”¹ådetach")
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
print(f"  ä¿®æ”¹å‰: {x}")
with torch.no_grad():
    x[0] += 10.0
x = x.detach()
print(f"  ä¿®æ”¹ådetach: {x}")

# æµ‹è¯•4: æ¨¡æ‹ŸJSMAçš„ä¿®æ”¹æ¨¡å¼
print("\næµ‹è¯•4: æ¨¡æ‹ŸJSMAå¾ªç¯")
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)
for i in range(3):
    print(f"\n  è¿­ä»£ {i+1}:")
    x.requires_grad = True
    print(f"    è®¾ç½®requires_grad=True: {x}")
    
    # æ¨¡æ‹Ÿå‰å‘ä¼ æ’­
    y = x.sum()
    y.backward()
    
    print(f"    æ¢¯åº¦: {x.grad}")
    
    # ä¿®æ”¹
    with torch.no_grad():
        x[i] += 10.0
    print(f"    ä¿®æ”¹å: {x}")
    
    # detach
    x = x.detach()
    print(f"    detachå: {x}")

print("\n" + "=" * 70)
print("âœ… æµ‹è¯•å®Œæˆ")
print("=" * 70)


