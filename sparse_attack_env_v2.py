# sparse_attack_env_v2.py
"""
ä¼˜åŒ–ç‰ˆæœ¬çš„ç¨€ç–æ”»å‡»ç¯å¢ƒ
ä¸»è¦æ”¹è¿›ï¼š
1. å¢å¼ºçŠ¶æ€è¡¨ç¤ºï¼ˆåŠ å…¥æ¢¯åº¦æ˜¾è‘—æ€§å›¾ï¼‰
2. æ”¹è¿›çš„å¥–åŠ±å‡½æ•°ï¼ˆç½®ä¿¡åº¦æ„ŸçŸ¥ï¼‰
3. ä¿®æ”¹å†å²è®°å½•
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import torch
import torch.nn.functional as F


class SparseAttackEnvV2(gym.Env):
    """
    ä¼˜åŒ–ç‰ˆæœ¬çš„ç¨€ç–å¯¹æŠ—æ”»å‡»ç¯å¢ƒ
    
    æ”¹è¿›ç‚¹ï¼š
    - çŠ¶æ€ç©ºé—´åŒ…å«æ¢¯åº¦æ˜¾è‘—æ€§å›¾
    - ç»†ç²’åº¦çš„ä¸­é—´å¥–åŠ±
    - ç½®ä¿¡åº¦æ„ŸçŸ¥çš„å¥–åŠ±è®¾è®¡
    """

    def __init__(self, clean_image, true_label, model, max_steps=5, 
                 use_saliency=True, confidence_reward_weight=5.0):
        """
        åˆå§‹åŒ–ç¯å¢ƒ
        
        å‚æ•°ï¼š
            clean_image: åŸå§‹å›¾åƒ [C, H, W]
            true_label: çœŸå®æ ‡ç­¾ï¼ˆæ•´æ•°ï¼‰
            model: ç›®æ ‡æ¨¡å‹ï¼ˆè¦æ”»å‡»çš„å¯¹è±¡ï¼‰
            max_steps: æœ€å¤§å…è®¸ä¿®æ”¹æ¬¡æ•°
            use_saliency: æ˜¯å¦ä½¿ç”¨æ˜¾è‘—æ€§å›¾å¢å¼ºçŠ¶æ€
            confidence_reward_weight: ç½®ä¿¡åº¦å¥–åŠ±çš„æƒé‡
        """
        super(SparseAttackEnvV2, self).__init__()

        # è®¾å¤‡ï¼ˆGPU/CPUï¼‰
        self.device = next(model.parameters()).device
        self.model = model.to(self.device)

        # æ”»å‡»é™åˆ¶
        self.max_steps = max_steps
        self.current_step = 0
        
        # é…ç½®
        self.use_saliency = use_saliency
        self.confidence_reward_weight = confidence_reward_weight

        # åŸå§‹å›¾åƒå¢åŠ  batch ç»´åº¦ -> [1, C, H, W]
        self.clean_image = clean_image.unsqueeze(0).to(self.device)
        self.true_label = true_label
        
        # å½“å‰å¯¹æŠ—å›¾åƒ
        self.current_image = self.clean_image.clone()
        
        # è·å–åˆå§‹é¢„æµ‹ç½®ä¿¡åº¦
        with torch.no_grad():
            output = self.model(self.current_image)
            self.initial_confidence = torch.softmax(output, dim=1)[0, true_label].item()
            self.prev_confidence = self.initial_confidence

        # è·å–å›¾åƒå°ºå¯¸
        _, C, H, W = self.current_image.shape
        self.height, self.width = H, W
        
        # ä¿®æ”¹å†å²æ©ç 
        self.modification_mask = torch.zeros((1, 1, H, W), device=self.device)

        # åŠ¨ä½œç©ºé—´ï¼š[x, y, dr, dg, db]
        self.action_space = spaces.Box(
            low=np.array([0, 0, -1.0, -1.0, -1.0]),
            high=np.array([W-1, H-1, 1.0, 1.0, 1.0]),
            dtype=np.float32
        )

        # çŠ¶æ€ç©ºé—´ï¼šå¢å¼ºç‰ˆ
        if use_saliency:
            # [å›¾åƒ(C) + æ˜¾è‘—æ€§å›¾(1) + ç½®ä¿¡åº¦å›¾(1) + ä¿®æ”¹æ©ç (1)]
            state_channels = C + 3
        else:
            state_channels = C
            
        self.observation_space = spaces.Box(
            low=-5, high=5, shape=(state_channels, H, W), dtype=np.float32
        )

        # CIFAR-10 å½’ä¸€åŒ–å‚æ•°
        self.mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(1, 3, 1, 1).to(self.device)
        self.std = torch.tensor([0.2023, 0.1994, 0.2010]).view(1, 3, 1, 1).to(self.device)

    def _compute_saliency_map(self):
        """
        è®¡ç®—æ¢¯åº¦æ˜¾è‘—æ€§å›¾
        æŒ‡å¯¼æ™ºèƒ½ä½“é€‰æ‹©æœ€æœ‰å½±å“åŠ›çš„åƒç´ 
        
        è¿”å›:
            saliency_map: [1, 1, H, W] æ˜¾è‘—æ€§å›¾
        """
        # éœ€è¦æ¢¯åº¦
        img = self.current_image.clone().detach().requires_grad_(True)
        
        # å‰å‘ä¼ æ’­
        output = self.model(img)
        
        # è®¡ç®—ç›®æ ‡ç±»åˆ«çš„æŸå¤±ï¼ˆæˆ‘ä»¬æƒ³é™ä½å®ƒï¼‰
        target_score = output[0, self.true_label]
        
        # åå‘ä¼ æ’­è·å–æ¢¯åº¦
        self.model.zero_grad()
        target_score.backward()
        
        # æ˜¾è‘—æ€§ = æ¢¯åº¦çš„ç»å¯¹å€¼
        saliency = torch.abs(img.grad)
        
        # èšåˆä¸‰ä¸ªé€šé“çš„æ˜¾è‘—æ€§
        saliency_map = saliency.sum(dim=1, keepdim=True)  # [1, 1, H, W]
        
        # å½’ä¸€åŒ–åˆ° [0, 1]
        saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min() + 1e-8)
        
        return saliency_map

    def _compute_confidence_map(self):
        """
        è®¡ç®—å½“å‰é¢„æµ‹çš„ç½®ä¿¡åº¦åˆ†å¸ƒå›¾
        
        è¿”å›:
            confidence_map: [1, 1, H, W] ç½®ä¿¡åº¦å›¾ï¼ˆå½“å‰ä¸ºå¸¸æ•°å›¾ï¼‰
        """
        with torch.no_grad():
            output = self.model(self.current_image)
            confidence = torch.softmax(output, dim=1)[0, self.true_label].item()
        
        # åˆ›å»ºå¸¸æ•°ç½®ä¿¡åº¦å›¾
        confidence_map = torch.ones((1, 1, self.height, self.width), device=self.device) * confidence
        
        return confidence_map

    def _get_observation(self):
        """
        è·å–å¢å¼ºçš„è§‚æµ‹çŠ¶æ€
        
        è¿”å›:
            obs: å¢å¼ºçš„çŠ¶æ€ [C+3, H, W]
        """
        if not self.use_saliency:
            # åªè¿”å›å›¾åƒ
            return self.current_image.squeeze(0).cpu().detach().numpy()
        
        # è®¡ç®—å¢å¼ºä¿¡æ¯
        saliency_map = self._compute_saliency_map()
        confidence_map = self._compute_confidence_map()
        
        # æ‹¼æ¥æ‰€æœ‰é€šé“
        # [å›¾åƒ(3) + æ˜¾è‘—æ€§(1) + ç½®ä¿¡åº¦(1) + ä¿®æ”¹æ©ç (1)] = (6, H, W)
        enhanced_state = torch.cat([
            self.current_image,      # [1, 3, H, W]
            saliency_map,            # [1, 1, H, W]
            confidence_map,          # [1, 1, H, W]
            self.modification_mask   # [1, 1, H, W]
        ], dim=1)  # -> [1, 6, H, W]
        
        return enhanced_state.squeeze(0).cpu().detach().numpy()

    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        if seed is not None:
            np.random.seed(seed)
            torch.manual_seed(seed)

        # é‡ç½®çŠ¶æ€
        self.current_image = self.clean_image.clone()
        self.current_step = 0
        self.modification_mask.zero_()
        
        # é‡ç½®ç½®ä¿¡åº¦
        with torch.no_grad():
            output = self.model(self.current_image)
            self.prev_confidence = torch.softmax(output, dim=1)[0, self.true_label].item()

        info = {}
        return self._get_observation(), info

    def step(self, action):
        """
        æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ
        
        æ”¹è¿›çš„å¥–åŠ±å‡½æ•°ï¼š
        - åŸºç¡€å¥–åŠ±ï¼ˆæˆåŠŸ/å¤±è´¥/æ­¥éª¤ï¼‰
        - ç½®ä¿¡åº¦å˜åŒ–å¥–åŠ±ï¼ˆä¸­é—´æŒ‡å¯¼ï¼‰
        - æ˜¾è‘—æ€§å¥–åŠ±ï¼ˆé¼“åŠ±ä¿®æ”¹é‡è¦åƒç´ ï¼‰
        """
        x, y, dr, dg, db = action
        x = int(np.clip(x, 0, self.width - 1))
        y = int(np.clip(y, 0, self.height - 1))
        
        # è®°å½•ä¿®æ”¹å‰çš„ç½®ä¿¡åº¦
        prev_conf = self.prev_confidence

        # ä¿®æ”¹åƒç´ 
        img_unnorm = self.current_image * self.std + self.mean
        delta = torch.tensor([[dr, dg, db]]).view(1, 3, 1, 1).to(self.device) / 255.0
        img_unnorm[:, :, y:y + 1, x:x + 1] += delta
        img_unnorm = torch.clamp(img_unnorm, 0, 1)
        self.current_image = (img_unnorm - self.mean) / self.std
        
        # æ›´æ–°ä¿®æ”¹æ©ç 
        self.modification_mask[:, :, y, x] = 1.0
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            logits = self.model(self.current_image)
            pred_label = logits.argmax(dim=1).item()
            current_conf = torch.softmax(logits, dim=1)[0, self.true_label].item()

        # åˆ¤æ–­æ”»å‡»æ˜¯å¦æˆåŠŸ
        success = (pred_label != self.true_label)
        self.current_step += 1
        done = success or (self.current_step >= self.max_steps)

        # ========== æ”¹è¿›çš„å¥–åŠ±å‡½æ•° ==========
        
        # 1. åŸºç¡€ç»ˆæ­¢å¥–åŠ±
        if success:
            reward = 10.0
        elif self.current_step >= self.max_steps:
            reward = -5.0
        else:
            reward = 0.0
        
        # 2. ç½®ä¿¡åº¦å˜åŒ–å¥–åŠ±ï¼ˆä¸­é—´æŒ‡å¯¼ï¼‰â­ æ ¸å¿ƒæ”¹è¿›
        confidence_delta = prev_conf - current_conf  # æ­£å€¼è¡¨ç¤ºç½®ä¿¡åº¦ä¸‹é™ï¼ˆå¥½äº‹ï¼‰
        reward += self.confidence_reward_weight * confidence_delta
        
        # 3. æ­¥éª¤æƒ©ç½šï¼ˆé¼“åŠ±ç¨€ç–æ€§ï¼‰
        reward -= 0.1
        
        # 4. æ˜¾è‘—æ€§å¥–åŠ±ï¼ˆå¯é€‰ï¼Œå¦‚æœä½¿ç”¨æ˜¾è‘—æ€§å›¾ï¼‰
        if self.use_saliency and not success:
            # å¦‚æœä¿®æ”¹äº†é«˜æ˜¾è‘—æ€§çš„åƒç´ ï¼Œç»™äºˆå°å¥–åŠ±
            saliency_map = self._compute_saliency_map()
            pixel_saliency = saliency_map[0, 0, y, x].item()
            reward += 0.5 * pixel_saliency  # é¼“åŠ±ä¿®æ”¹é‡è¦åƒç´ 
        
        # æ›´æ–°ç½®ä¿¡åº¦è®°å½•
        self.prev_confidence = current_conf

        # ä¿¡æ¯è®°å½•
        info = {
            'success': success,
            'modified_pixel': (x, y),
            'current_pred': pred_label,
            'confidence': current_conf,
            'confidence_delta': confidence_delta,
            'step': self.current_step,
            'reward_breakdown': {
                'base': 10.0 if success else (-5.0 if done else 0.0),
                'confidence': self.confidence_reward_weight * confidence_delta,
                'step_penalty': -0.1,
                'saliency': 0.5 * pixel_saliency if (self.use_saliency and not success) else 0.0
            }
        }

        obs = self._get_observation()
        terminated = success
        truncated = (self.current_step >= self.max_steps) and not success

        return obs, reward, terminated, truncated, info

    def render(self):
        """æ˜¾ç¤ºå›¾åƒï¼ˆå¯é€‰ï¼‰"""
        pass


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯• SparseAttackEnvV2")
    
    from torchvision import datasets, transforms
    from target_model import load_target_model
    
    # åŠ è½½æ•°æ®å’Œæ¨¡å‹
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    model = load_target_model('resnet18', num_classes=10)
    
    image, label = test_set[0]
    
    # åˆ›å»ºä¼˜åŒ–ç‰ˆç¯å¢ƒ
    env = SparseAttackEnvV2(image, label, model, max_steps=5, use_saliency=True)
    
    print(f"çŠ¶æ€ç©ºé—´ç»´åº¦: {env.observation_space.shape}")
    print(f"åŠ¨ä½œç©ºé—´ç»´åº¦: {env.action_space.shape}")
    
    # æµ‹è¯•ä¸€æ­¥
    obs, info = env.reset()
    print(f"\nåˆå§‹çŠ¶æ€å½¢çŠ¶: {obs.shape}")
    print(f"åˆå§‹ç½®ä¿¡åº¦: {info}")
    
    action = env.action_space.sample()
    obs, reward, terminated, truncated, info = env.step(action)
    
    print(f"\næ‰§è¡ŒåŠ¨ä½œå:")
    print(f"å¥–åŠ±: {reward:.4f}")
    print(f"å¥–åŠ±åˆ†è§£: {info['reward_breakdown']}")
    print(f"ç½®ä¿¡åº¦å˜åŒ–: {info['confidence_delta']:.4f}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")

