# 📅 今日任务 - Week 1 Day 3

**日期**：2025年11月5日  
**目标**：优化RL方法 - 实现多样本训练

---

## 🎯 策略调整说明

**原计划**：Day 3训练MobileNetV2 + 运行实验  
**新策略**：先优化RL，再做完整对比实验 ⭐

**原因**：
- ✅ 避免重复实验，节省时间
- ✅ 一次性得到完整、漂亮的结果
- ✅ RL方法不会"垫底"
- ✅ 更有条理，更符合科研逻辑

---

## 📋 今天的任务

### 核心目标：让RL方法在ResNet18上达到70%+ ASR

### 任务1：完善多样本训练脚本（1-2小时）

**已有基础**：
- ✅ `ppo_trainer_v3_improved.py` 已创建
- ✅ 核心逻辑已实现

**需要做**：
1. 测试脚本是否能运行
2. 修复可能的bug
3. 添加进度显示和日志

**测试命令**：
```bash
# 快速验证（少量样本和步数）
python ppo_trainer_v3_improved.py
```

**预期输出**：
- 正常加载数据
- 正常创建环境
- 开始训练（不报错）

---

### 任务2：在ResNet18上训练改进的RL agent（1-2小时）

**训练配置**：
```bash
python train_model_specific_agent.py \
    --model resnet18 \
    --model_path cifar10_resnet18.pth \
    --num_samples 100 \
    --timesteps 50000 \
    --save_path models/ppo_resnet18_v3
```

**参数说明**：
- `num_samples=100`：用100个不同样本训练
- `timesteps=50000`：训练5万步（约1-2小时）
- 每次reset随机选择新样本

**预期训练时间**：1-2小时（GPU）

**预期结果**：
- 训练收敛
- 奖励值上升
- 保存模型到 `models/ppo_resnet18_v3.zip`

---

### 任务3：快速验证改进效果（30分钟）

**测试脚本**：
```python
# 创建 quick_test_rl_v3.py
# 在20-50个样本上快速测试
python quick_test_rl_v3.py \
    --model resnet18 \
    --agent_path models/ppo_resnet18_v3.zip \
    --num_samples 50
```

**预期ASR**：
- 目标：≥ 70%
- 如果 < 60%：需要调整训练参数
- 如果 ≥ 70%：成功！✅

**对比**：
```
改进前（V2，单样本训练）: ASR = 30-40%
改进后（V3，多样本训练）: ASR = 70%+  ← 提升40%！
```

---

### 任务4：分析和调试（如果需要）

**如果ASR < 60%**，检查：

1. **训练是否收敛？**
   ```bash
   # 查看TensorBoard日志
   tensorboard --logdir ./logs/
   ```
   - 奖励值是否上升？
   - 是否震荡过大？

2. **样本是否多样？**
   - 100个样本是否都被用到？
   - 是否每次reset都换样本？

3. **超参数是否合适？**
   - 学习率：3e-4
   - 熵系数：0.01
   - 可能需要调整

**调整策略**：
- 增加训练步数：50k → 100k
- 增加训练样本：100 → 200
- 调整学习率：3e-4 → 1e-4

---

## 📊 成功标准

### 必须达到：
- ✅ 训练脚本运行成功
- ✅ ResNet18 RL agent训练完成
- ✅ ASR ≥ 60%（最低要求）

### 理想目标：
- ⭐ ASR ≥ 70%
- ⭐ 优于JSMA（55%）
- ⭐ 优于SparseFool（47%）

---

## 🔄 如果时间充足（可选）

### 选项A：继续优化
- 调整超参数
- 增加训练时间
- 冲击ASR 80%+

### 选项B：开始VGG16 RL训练
- 为VGG16训练专门的agent
- 预期ASR 60-70%

### 选项C：实现可视化
- 训练曲线可视化
- 攻击成功案例可视化

**建议**：如果ASR达到70%，选B（继续VGG16）

---

## 💡 关键点

### 1. 多样本训练的核心

**原理**：
```python
# 旧方法（单样本）
image, label = dataset[0]  # 只用一张图
env = Env(image, label)
agent.learn(10000)  # 在这张图上学10000步 ❌

# 新方法（多样本）
samples = [dataset[i] for i in range(100)]  # 100张图
def reset():
    sample = random.choice(samples)  # 每次随机选
    return Env(sample)

agent.learn(50000)  # 在100张不同的图上学 ✅
```

**为什么有效**？
- 学到通用策略，而非记住特定图片
- 泛化能力强
- ASR大幅提升

### 2. 模型特定训练

**为什么需要**？
- ResNet18和VGG16梯度分布不同
- 显著性图完全不同
- 需要分别训练

**论文价值**？
- 不是缺点，是深入分析！
- 可以对比"通用agent vs 专用agent"
- 增加论文深度

---

## 📝 预期产出

### 今天结束时应该有：

1. ✅ 可运行的多样本训练脚本
2. ✅ ResNet18的改进RL agent (V3)
3. ✅ 验证结果：ASR ≥ 70%
4. ✅ 对比数据：V2 vs V3
5. ✅ 明确Day 4-7的计划

---

## 🔔 明天预告 - Day 4

**主要任务**：
1. 完整测试ResNet18 RL V3（100-200样本）
2. 开始VGG16 RL V3训练
3. 准备MobileNetV2模型训练

**时间安排**：
- 上午：ResNet18完整测试（2小时）
- 下午：VGG16 RL训练（1-2小时）
- 晚上：分析结果，规划调整

---

## 💪 今天的重要性

**Day 3是转折点**！

- ✅ 如果成功：RL方法从垫底→领先
- ✅ ASR提升40%：这是论文的核心贡献
- ✅ 为后续实验打好基础
- ✅ 证明我们的方法有效

**这一天的工作将决定论文质量！**

---

## 🎯 加油！

记住：
1. **不要急** - 慢慢调试，确保正确
2. **多测试** - 每一步都验证
3. **记录日志** - 记录训练参数和结果
4. **有问题随时问** - 我随时帮你

**你能做到的！** 🚀

---

*生成时间：2025-11-04*
*策略调整：优先优化RL，再做完整对比*








