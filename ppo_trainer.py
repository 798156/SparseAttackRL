# ppo_trainer.py
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

def train_rl_agent(env, timesteps=5000, save_path="ppo_sparse_model"):
    """
    ä½¿ç”¨ MlpPolicy è®­ç»ƒ RL æ™ºèƒ½ä½“ï¼ˆé€‚ç”¨äºå°å›¾åƒè¾“å…¥ï¼‰
    """
    vec_env = DummyVecEnv([lambda: env])

    model = PPO(
        policy="MlpPolicy",           # âœ… ä½¿ç”¨ MLP å¤„ç†å±•å¹³åçš„å›¾åƒ
        env=vec_env,
        verbose=1,
        tensorboard_log="./logs/",
        learning_rate=3e-4,
        gamma=0.99,
        ent_coef=0.01,
        batch_size=64,
        device="auto"  # è‡ªåŠ¨ä½¿ç”¨ cpu/cuda
    )

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ RL æ™ºèƒ½ä½“ï¼Œå…± {timesteps} æ­¥...")
    model.learn(total_timesteps=timesteps, tb_log_name="ppo_run")
    model.save(save_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}.zip")

    return model
