# evaluation_metrics.py
"""
æ‰©å±•çš„è¯„ä¼°æŒ‡æ ‡è®¡ç®—
ç”¨äºè®ºæ–‡å®éªŒçš„å…¨é¢è¯„ä¼°
"""

import torch
import numpy as np
from skimage.metrics import structural_similarity as ssim
from skimage.metrics import peak_signal_noise_ratio as psnr


def compute_l0_norm(original, adversarial, threshold=1e-6):
    """
    è®¡ç®—L0èŒƒæ•°ï¼šä¿®æ”¹çš„åƒç´ æ•°é‡
    
    Args:
        original: åŸå§‹å›¾åƒ (C, H, W)
        adversarial: å¯¹æŠ—å›¾åƒ (C, H, W)
        threshold: åˆ¤å®šåƒç´ è¢«ä¿®æ”¹çš„é˜ˆå€¼
    
    Returns:
        l0: ä¿®æ”¹çš„åƒç´ æ•°
    """
    # ä½¿ç”¨é˜ˆå€¼åˆ¤æ–­ï¼Œè€Œä¸æ˜¯ä¸¥æ ¼çš„ä¸ç­‰äºï¼ˆé¿å…æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜ï¼‰
    diff = torch.abs(original - adversarial)
    # å¦‚æœä»»ä½•é€šé“çš„å·®å¼‚è¶…è¿‡é˜ˆå€¼ï¼Œè¯¥åƒç´ å°±ç®—è¢«ä¿®æ”¹
    l0 = (diff.sum(dim=0) > threshold).sum().item()
    return l0


def compute_l2_norm(original, adversarial):
    """
    è®¡ç®—L2èŒƒæ•°ï¼šæ‰°åŠ¨çš„æ¬§å¼è·ç¦»
    
    Args:
        original: åŸå§‹å›¾åƒ (C, H, W)
        adversarial: å¯¹æŠ—å›¾åƒ (C, H, W)
    
    Returns:
        l2: L2èŒƒæ•°
    """
    diff = (original - adversarial).flatten()
    l2 = torch.norm(diff, p=2).item()
    return l2


def compute_linf_norm(original, adversarial):
    """
    è®¡ç®—LâˆèŒƒæ•°ï¼šæœ€å¤§å•åƒç´ æ‰°åŠ¨
    
    Args:
        original: åŸå§‹å›¾åƒ (C, H, W)
        adversarial: å¯¹æŠ—å›¾åƒ (C, H, W)
    
    Returns:
        linf: LâˆèŒƒæ•°
    """
    diff = torch.abs(original - adversarial)
    linf = diff.max().item()
    return linf


def compute_ssim(original, adversarial):
    """
    è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦ (SSIM)
    å€¼è¶Šæ¥è¿‘1è¡¨ç¤ºå›¾åƒè¶Šç›¸ä¼¼
    
    Args:
        original: åŸå§‹å›¾åƒ (C, H, W) Tensor
        adversarial: å¯¹æŠ—å›¾åƒ (C, H, W) Tensor
    
    Returns:
        ssim_value: SSIMå€¼ (0-1)
    """
    # è½¬æ¢ä¸ºnumpyå¹¶è°ƒæ•´ç»´åº¦é¡ºåº (H, W, C)
    orig_np = original.cpu().numpy().transpose(1, 2, 0)
    adv_np = adversarial.cpu().numpy().transpose(1, 2, 0)
    
    # å½’ä¸€åŒ–åˆ°[0, 1]èŒƒå›´
    orig_np = (orig_np - orig_np.min()) / (orig_np.max() - orig_np.min() + 1e-8)
    adv_np = (adv_np - adv_np.min()) / (adv_np.max() - adv_np.min() + 1e-8)
    
    # è®¡ç®—SSIM
    ssim_value = ssim(orig_np, adv_np, multichannel=True, channel_axis=2, data_range=1.0)
    
    return ssim_value


def compute_psnr(original, adversarial):
    """
    è®¡ç®—å³°å€¼ä¿¡å™ªæ¯” (PSNR)
    å€¼è¶Šå¤§è¡¨ç¤ºå›¾åƒè´¨é‡è¶Šå¥½
    
    Args:
        original: åŸå§‹å›¾åƒ (C, H, W) Tensor
        adversarial: å¯¹æŠ—å›¾åƒ (C, H, W) Tensor
    
    Returns:
        psnr_value: PSNRå€¼ (dB)
    """
    # è½¬æ¢ä¸ºnumpy
    orig_np = original.cpu().numpy().transpose(1, 2, 0)
    adv_np = adversarial.cpu().numpy().transpose(1, 2, 0)
    
    # å½’ä¸€åŒ–åˆ°[0, 1]èŒƒå›´
    orig_np = (orig_np - orig_np.min()) / (orig_np.max() - orig_np.min() + 1e-8)
    adv_np = (adv_np - adv_np.min()) / (adv_np.max() - adv_np.min() + 1e-8)
    
    # è®¡ç®—PSNR
    psnr_value = psnr(orig_np, adv_np, data_range=1.0)
    
    return psnr_value


def compute_all_metrics(original, adversarial):
    """
    è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
    
    Args:
        original: åŸå§‹å›¾åƒ (C, H, W)
        adversarial: å¯¹æŠ—å›¾åƒ (C, H, W)
    
    Returns:
        metrics: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
    """
    metrics = {
        'l0_norm': compute_l0_norm(original, adversarial),
        'l2_norm': compute_l2_norm(original, adversarial),
        'linf_norm': compute_linf_norm(original, adversarial),
        'ssim': compute_ssim(original, adversarial),
        'psnr': compute_psnr(original, adversarial)
    }
    
    return metrics


def compute_query_efficiency(num_queries, success):
    """
    è®¡ç®—æŸ¥è¯¢æ•ˆç‡
    
    Args:
        num_queries: æŸ¥è¯¢æ¬¡æ•°
        success: æ˜¯å¦æˆåŠŸ
    
    Returns:
        efficiency: æ•ˆç‡åˆ†æ•°ï¼ˆæˆåŠŸæ—¶ä¸º1/queriesï¼Œå¤±è´¥æ—¶ä¸º0ï¼‰
    """
    if success:
        return 1.0 / num_queries
    else:
        return 0.0


class MetricsAggregator:
    """èšåˆå¤šä¸ªæ ·æœ¬çš„æŒ‡æ ‡ç»Ÿè®¡"""
    
    def __init__(self):
        self.metrics = {
            'l0_norm': [],
            'l2_norm': [],
            'linf_norm': [],
            'ssim': [],
            'psnr': [],
            'query_count': [],
            'attack_time': [],
            'success': []
        }
    
    def add(self, **kwargs):
        """æ·»åŠ ä¸€ä¸ªæ ·æœ¬çš„æŒ‡æ ‡"""
        for key, value in kwargs.items():
            if key in self.metrics:
                self.metrics[key].append(value)
    
    def compute_statistics(self):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        for metric_name, values in self.metrics.items():
            if not values:
                continue
            
            if metric_name == 'success':
                # å¸ƒå°”å€¼ï¼šè®¡ç®—æˆåŠŸç‡
                stats[f'{metric_name}_rate'] = np.mean(values)
            else:
                # æ•°å€¼ï¼šè®¡ç®—å‡å€¼ã€æ ‡å‡†å·®ã€ä¸­ä½æ•°
                successful_values = [v for i, v in enumerate(values) 
                                   if self.metrics['success'][i]]
                
                if successful_values:
                    stats[f'{metric_name}_mean'] = np.mean(successful_values)
                    stats[f'{metric_name}_std'] = np.std(successful_values)
                    stats[f'{metric_name}_median'] = np.median(successful_values)
                    stats[f'{metric_name}_min'] = np.min(successful_values)
                    stats[f'{metric_name}_max'] = np.max(successful_values)
        
        return stats
    
    def save_to_csv(self, filename):
        """ä¿å­˜åŸå§‹æ•°æ®åˆ°CSV"""
        import pandas as pd
        
        # ç¡®ä¿æ‰€æœ‰åˆ—é•¿åº¦ä¸€è‡´ï¼ˆç”¨Noneå¡«å……ç¼ºå¤±å€¼ï¼‰
        max_len = max(len(v) for v in self.metrics.values() if v)
        
        aligned_metrics = {}
        for key, values in self.metrics.items():
            if len(values) < max_len:
                # ç”¨Noneå¡«å……åˆ°æœ€å¤§é•¿åº¦
                aligned_metrics[key] = values + [None] * (max_len - len(values))
            else:
                aligned_metrics[key] = values
        
        df = pd.DataFrame(aligned_metrics)
        df.to_csv(filename, index=False)
        print(f"âœ… æŒ‡æ ‡æ•°æ®å·²ä¿å­˜è‡³: {filename}")


def statistical_significance_test(method1_results, method2_results, metric='asr'):
    """
    ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆé…å¯¹tæ£€éªŒï¼‰
    
    Args:
        method1_results: æ–¹æ³•1çš„ç»“æœåˆ—è¡¨ï¼ˆå¸ƒå°”æˆ–æ•°å€¼ï¼‰
        method2_results: æ–¹æ³•2çš„ç»“æœåˆ—è¡¨ï¼ˆå¸ƒå°”æˆ–æ•°å€¼ï¼‰
        metric: è¯„ä¼°æŒ‡æ ‡åç§°
    
    Returns:
        result: åŒ…å«tç»Ÿè®¡é‡ã€på€¼ã€æ˜¯å¦æ˜¾è‘—çš„å­—å…¸
    """
    from scipy import stats
    
    # è½¬æ¢ä¸ºæ•°å€¼æ•°ç»„ï¼ˆå¦‚æœæ˜¯å¸ƒå°”ç±»å‹ï¼‰
    method1_array = np.array(method1_results, dtype=float)
    method2_array = np.array(method2_results, dtype=float)
    
    # é…å¯¹tæ£€éªŒ
    t_stat, p_value = stats.ttest_rel(method1_array, method2_array)
    
    # åˆ¤æ–­æ˜¾è‘—æ€§ï¼ˆé€šå¸¸ä½¿ç”¨ Î±=0.05ï¼‰
    is_significant = p_value < 0.05
    
    result = {
        'metric': metric,
        't_statistic': t_stat,
        'p_value': p_value,
        'is_significant': is_significant,
        'alpha': 0.05
    }
    
    # æ‰“å°ç»“æœ
    print(f"\nğŸ“Š {metric} çš„ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ:")
    print(f"   tç»Ÿè®¡é‡: {t_stat:.4f}")
    print(f"   på€¼: {p_value:.4f}")
    print(f"   æ˜¾è‘—æ€§(Î±=0.05): {'âœ… æ˜¾è‘—' if is_significant else 'âŒ ä¸æ˜¾è‘—'}")
    
    return result


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ“Š è¯„ä¼°æŒ‡æ ‡æ¨¡å—")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿç¤ºä¾‹
    original = torch.randn(3, 32, 32)
    adversarial = original.clone()
    adversarial[:, 10, 10] += 0.5  # ä¿®æ”¹ä¸€ä¸ªåƒç´ 
    
    # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    metrics = compute_all_metrics(original, adversarial)
    
    print("\næŒ‡æ ‡è®¡ç®—ç»“æœ:")
    for name, value in metrics.items():
        print(f"  {name}: {value:.4f}")
    
    # èšåˆå™¨ç¤ºä¾‹
    print("\nèšåˆå™¨ç¤ºä¾‹:")
    aggregator = MetricsAggregator()
    
    # æ·»åŠ å‡ ä¸ªæ ·æœ¬
    for i in range(5):
        aggregator.add(
            l0_norm=i+1,
            l2_norm=np.random.rand(),
            success=i % 2 == 0,
            attack_time=np.random.rand() * 10
        )
    
    # è®¡ç®—ç»Ÿè®¡
    stats = aggregator.compute_statistics()
    print("\nç»Ÿè®¡ç»“æœ:")
    for name, value in stats.items():
        print(f"  {name}: {value:.4f}")



