# debug_jsma_detailed.py
"""è¯¦ç»†è°ƒè¯•JSMAæ”»å‡»ï¼ŒæŸ¥çœ‹æ¯ä¸€æ­¥çš„å˜åŒ–"""

import torch
import torchvision
import torchvision.transforms as transforms
from target_model import load_target_model
from jsma_attack import jsma_attack
from evaluation_metrics import compute_l0_norm
import torch.nn.functional as F

# åŠ è½½æ¨¡å‹å’Œæ•°æ®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = load_target_model("resnet18")
model = model.to(device)
model.eval()

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

print("=" * 80)
print("ğŸ”¬ è¯¦ç»†è°ƒè¯•JSMAæ”»å‡»è¿‡ç¨‹")
print("=" * 80)

# æ‰¾ä¸€ä¸ªæ¨¡å‹é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬
idx = 0
while idx < 100:
    image, label = testset[idx]
    
    with torch.no_grad():
        output = model(image.unsqueeze(0).to(device))
        pred = output.argmax(dim=1).item()
        probs = F.softmax(output, dim=1)[0]
        confidence = probs[pred].item()
        
        if pred == label and confidence > 0.5:
            break
    idx += 1

if idx >= 100:
    print("âŒ æ‰¾ä¸åˆ°åˆé€‚çš„æµ‹è¯•æ ·æœ¬")
    exit(1)

print(f"\nğŸ“ æµ‹è¯•æ ·æœ¬ #{idx}")
print(f"  çœŸå®æ ‡ç­¾: {label}")
print(f"  æ¨¡å‹é¢„æµ‹: {pred}")
print(f"  ç½®ä¿¡åº¦: {confidence:.4f}")

# æ‰‹åŠ¨æ‰§è¡ŒJSMAæ”»å‡»å¹¶è®°å½•æ¯ä¸€æ­¥
print("\n" + "=" * 80)
print("ğŸ¯ å¼€å§‹JSMAæ”»å‡» (theta=10.0, max_pixels=5)")
print("=" * 80)

adv_image = image.clone().to(device)
adv_image_batch = adv_image.unsqueeze(0)

for step in range(5):
    print(f"\n--- æ­¥éª¤ {step + 1} ---")
    
    # æ£€æŸ¥å½“å‰é¢„æµ‹
    with torch.no_grad():
        output = model(adv_image_batch)
        pred = output.argmax(dim=1).item()
        probs = F.softmax(output, dim=1)[0]
        confidence = probs[pred].item()
        top5 = torch.topk(probs, 5)
        
        print(f"  å½“å‰é¢„æµ‹: {pred} (ç½®ä¿¡åº¦: {confidence:.4f})")
        print(f"  Top-5 é¢„æµ‹: {top5.indices.tolist()}")
        print(f"  Top-5 ç½®ä¿¡åº¦: {[f'{p:.4f}' for p in top5.values.tolist()]}")
        
        if pred != label:
            print(f"  âœ… æ”»å‡»æˆåŠŸï¼é¢„æµ‹ä» {label} å˜ä¸º {pred}")
            break
    
    # è®¡ç®—æ¢¯åº¦
    adv_image_batch.requires_grad_(True)
    output = model(adv_image_batch)
    
    model.zero_grad()
    output[0, label].backward()
    grad = adv_image_batch.grad
    
    # æ‰¾åˆ°æ¢¯åº¦æœ€å¤§çš„ä½ç½®
    grad_abs = torch.abs(grad[0])
    max_val = grad_abs.max().item()
    max_idx = grad_abs.argmax().item()
    
    C, H, W = adv_image.shape
    c = max_idx // (H * W)
    h = (max_idx % (H * W)) // W
    w = max_idx % W
    
    print(f"  é€‰ä¸­åƒç´ : channel={c}, h={h}, w={w}")
    print(f"  æ¢¯åº¦å€¼: {grad[0, c, h, w].item():.6f}")
    print(f"  åŸå§‹å€¼: {adv_image[c, h, w].item():.6f}")
    
    # åº”ç”¨æ‰°åŠ¨ (theta=10.0)
    with torch.no_grad():
        direction = -torch.sign(grad[0, c, h, w])
        adv_image[c, h, w] += direction * 10.0
        adv_image_batch = adv_image.unsqueeze(0)
    
    print(f"  ä¿®æ”¹å: {adv_image[c, h, w].item():.6f}")
    print(f"  å˜åŒ–é‡: {direction.item() * 10.0:.6f}")

# æœ€ç»ˆç»“æœ
print("\n" + "=" * 80)
print("ğŸ“Š æœ€ç»ˆç»“æœ")
print("=" * 80)

with torch.no_grad():
    output = model(adv_image_batch)
    pred_final = output.argmax(dim=1).item()
    probs_final = F.softmax(output, dim=1)[0]
    confidence_final = probs_final[pred_final].item()

l0 = compute_l0_norm(image.cpu(), adv_image.cpu())

print(f"  åŸå§‹é¢„æµ‹: {label}")
print(f"  æœ€ç»ˆé¢„æµ‹: {pred_final}")
print(f"  æœ€ç»ˆç½®ä¿¡åº¦: {confidence_final:.4f}")
print(f"  L0èŒƒæ•°: {l0}")
print(f"  æ”»å‡»ç»“æœ: {'âœ… æˆåŠŸ' if pred_final != label else 'âŒ å¤±è´¥'}")

# ä½¿ç”¨å®˜æ–¹JSMAå‡½æ•°éªŒè¯
print("\n" + "=" * 80)
print("ğŸ” ä½¿ç”¨å®˜æ–¹JSMAå‡½æ•°éªŒè¯")
print("=" * 80)

success_official, adv_official, pixels_official = jsma_attack(
    image.to(device), label, model, max_pixels=5, theta=10.0
)

l0_official = compute_l0_norm(image.cpu(), adv_official.cpu())

with torch.no_grad():
    output_official = model(adv_official.unsqueeze(0).to(device))
    pred_official = output_official.argmax(dim=1).item()

print(f"  Successæ ‡å¿—: {success_official}")
print(f"  ä¿®æ”¹åƒç´ æ•°: {len(pixels_official)}")
print(f"  æœ€ç»ˆé¢„æµ‹: {pred_official}")
print(f"  L0èŒƒæ•°: {l0_official}")
print(f"  çœŸå®æˆåŠŸ: {pred_official != label}")
