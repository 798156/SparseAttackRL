"""
ç»Ÿè®¡åˆ†æè„šæœ¬ - Day 6
å¯¹æ‰€æœ‰å®éªŒç»“æœè¿›è¡Œæ·±å…¥çš„ç»Ÿè®¡åˆ†æ
"""

import json
import numpy as np
from scipy import stats
from pathlib import Path
import pandas as pd
from typing import Dict, List, Tuple

def load_results() -> Dict:
    """åŠ è½½æ‰€æœ‰å®éªŒç»“æœ"""
    results = {
        'ResNet18': json.load(open('results/week1_day1/resnet18_summary.json')),
        'VGG16': json.load(open('results/week1_day2/vgg16_summary.json')),
        'MobileNetV2': json.load(open('results/week1_day5/mobilenetv2_summary.json'))
    }
    return results

def load_detailed_results() -> Dict:
    """åŠ è½½è¯¦ç»†çš„å•æ ·æœ¬ç»“æœ"""
    detailed = {}
    
    # æ£€æŸ¥å¹¶åŠ è½½æ‰€æœ‰å¯èƒ½çš„è¯¦ç»†æ•°æ®æ–‡ä»¶
    detailed_files = {
        'ResNet18': 'results/week1_day1/resnet18_detailed.json',
        'VGG16': 'results/week1_day2/vgg16_detailed.json',
        'MobileNetV2': 'results/week1_day5/mobilenetv2_detailed.json'
    }
    
    for model, filepath in detailed_files.items():
        if Path(filepath).exists():
            with open(filepath, 'r') as f:
                detailed[model] = json.load(f)
            print(f"  âœ… {model}: è¯¦ç»†æ•°æ®åŠ è½½æˆåŠŸ")
        else:
            print(f"  âš ï¸  {model}: è¯¦ç»†æ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡")
    
    return detailed

def extract_attack_results(detailed_data: Dict, attack_name: str) -> List[bool]:
    """æå–æŸä¸ªæ”»å‡»æ–¹æ³•çš„æˆåŠŸ/å¤±è´¥åˆ—è¡¨"""
    results = []
    for sample in detailed_data:
        if attack_name in sample:
            results.append(sample[attack_name]['success'])
    return results

def significance_test_between_methods(detailed: Dict, model: str) -> pd.DataFrame:
    """å¯¹åŒä¸€æ¨¡å‹çš„ä¸åŒæ”»å‡»æ–¹æ³•è¿›è¡Œæ˜¾è‘—æ€§æ£€éªŒ"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š {model} - æ”»å‡»æ–¹æ³•ä¹‹é—´çš„æ˜¾è‘—æ€§æ£€éªŒ")
    print(f"{'='*80}\n")
    
    methods = ['JSMA', 'One-Pixel', 'SparseFool']
    
    # æå–æˆåŠŸç‡æ•°æ®
    method_success = {}
    for method in methods:
        success_list = extract_attack_results(detailed[model], method)
        method_success[method] = success_list
        print(f"{method}: {len(success_list)}ä¸ªæ ·æœ¬, ASR={sum(success_list)/len(success_list)*100:.1f}%")
    
    # ä¸¤ä¸¤æ¯”è¾ƒ (McNemar's test - é…å¯¹æ ·æœ¬)
    results = []
    for i in range(len(methods)):
        for j in range(i+1, len(methods)):
            method1, method2 = methods[i], methods[j]
            success1 = method_success[method1]
            success2 = method_success[method2]
            
            # McNemar's test (é€‚ç”¨äºé…å¯¹äºŒåˆ†ç±»æ•°æ®)
            # æ„å»ºæ··æ·†çŸ©é˜µ
            both_success = sum(s1 and s2 for s1, s2 in zip(success1, success2))
            both_fail = sum(not s1 and not s2 for s1, s2 in zip(success1, success2))
            only_1 = sum(s1 and not s2 for s1, s2 in zip(success1, success2))
            only_2 = sum(not s1 and s2 for s1, s2 in zip(success1, success2))
            
            # McNemarç»Ÿè®¡é‡
            if only_1 + only_2 > 0:
                statistic = (abs(only_1 - only_2) - 1) ** 2 / (only_1 + only_2)
                p_value = 1 - stats.chi2.cdf(statistic, 1)
            else:
                statistic = 0
                p_value = 1.0
            
            significance = 'âœ… æ˜¾è‘—' if p_value < 0.05 else 'âŒ ä¸æ˜¾è‘—'
            
            results.append({
                'å¯¹æ¯”': f'{method1} vs {method2}',
                'ç»Ÿè®¡é‡': f'{statistic:.3f}',
                'på€¼': f'{p_value:.4f}',
                'ç»“è®º': significance
            })
            
            print(f"\n{method1} vs {method2}:")
            print(f"  ä¸¤è€…éƒ½æˆåŠŸ: {both_success}")
            print(f"  ä¸¤è€…éƒ½å¤±è´¥: {both_fail}")
            print(f"  ä»…{method1}æˆåŠŸ: {only_1}")
            print(f"  ä»…{method2}æˆåŠŸ: {only_2}")
            print(f"  McNemarç»Ÿè®¡é‡: {statistic:.3f}")
            print(f"  på€¼: {p_value:.4f} {significance}")
    
    return pd.DataFrame(results)

def significance_test_between_models(detailed: Dict, method: str) -> pd.DataFrame:
    """å¯¹åŒä¸€æ”»å‡»æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„è¡¨ç°è¿›è¡Œæ˜¾è‘—æ€§æ£€éªŒ"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š {method} - ä¸åŒæ¨¡å‹ä¹‹é—´çš„æ˜¾è‘—æ€§æ£€éªŒ")
    print(f"{'='*80}\n")
    
    models = ['ResNet18', 'VGG16', 'MobileNetV2']
    
    # æå–æˆåŠŸç‡æ•°æ®
    model_success = {}
    for model in models:
        if model in detailed:
            success_list = extract_attack_results(detailed[model], method)
            model_success[model] = success_list
            print(f"{model}: {len(success_list)}ä¸ªæ ·æœ¬, ASR={sum(success_list)/len(success_list)*100:.1f}%")
    
    # ä¸¤ä¸¤æ¯”è¾ƒ
    results = []
    model_list = list(model_success.keys())
    for i in range(len(model_list)):
        for j in range(i+1, len(model_list)):
            model1, model2 = model_list[i], model_list[j]
            success1 = model_success[model1]
            success2 = model_success[model2]
            
            # ç¡®ä¿æ ·æœ¬æ•°ç›¸åŒ
            min_len = min(len(success1), len(success2))
            success1 = success1[:min_len]
            success2 = success2[:min_len]
            
            # McNemar's test
            both_success = sum(s1 and s2 for s1, s2 in zip(success1, success2))
            both_fail = sum(not s1 and not s2 for s1, s2 in zip(success1, success2))
            only_1 = sum(s1 and not s2 for s1, s2 in zip(success1, success2))
            only_2 = sum(not s1 and s2 for s1, s2 in zip(success1, success2))
            
            if only_1 + only_2 > 0:
                statistic = (abs(only_1 - only_2) - 1) ** 2 / (only_1 + only_2)
                p_value = 1 - stats.chi2.cdf(statistic, 1)
            else:
                statistic = 0
                p_value = 1.0
            
            significance = 'âœ… æ˜¾è‘—' if p_value < 0.05 else 'âŒ ä¸æ˜¾è‘—'
            
            results.append({
                'å¯¹æ¯”': f'{model1} vs {model2}',
                'ç»Ÿè®¡é‡': f'{statistic:.3f}',
                'på€¼': f'{p_value:.4f}',
                'ç»“è®º': significance
            })
            
            print(f"\n{model1} vs {model2}:")
            print(f"  ä¸¤è€…éƒ½æˆåŠŸ: {both_success}")
            print(f"  ä¸¤è€…éƒ½å¤±è´¥: {both_fail}")
            print(f"  ä»…{model1}æˆåŠŸ: {only_1}")
            print(f"  ä»…{model2}æˆåŠŸ: {only_2}")
            print(f"  McNemarç»Ÿè®¡é‡: {statistic:.3f}")
            print(f"  på€¼: {p_value:.4f} {significance}")
    
    return pd.DataFrame(results)

def analyze_failure_cases(detailed: Dict) -> Dict:
    """åˆ†æå¤±è´¥æ¡ˆä¾‹"""
    print(f"\n{'='*80}")
    print(f"ğŸ” å¤±è´¥æ¡ˆä¾‹åˆ†æ")
    print(f"{'='*80}\n")
    
    failure_stats = {}
    
    for model in detailed.keys():
        print(f"\nã€{model}ã€‘")
        model_data = detailed[model]
        
        # ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬çš„å¤±è´¥æƒ…å†µ
        all_fail = 0  # æ‰€æœ‰æ”»å‡»éƒ½å¤±è´¥
        partial_fail = 0  # éƒ¨åˆ†æ”»å‡»å¤±è´¥
        all_success = 0  # æ‰€æœ‰æ”»å‡»éƒ½æˆåŠŸ
        
        all_fail_samples = []
        
        for sample in model_data:
            methods = ['JSMA', 'One-Pixel', 'SparseFool']
            success_count = sum(1 for m in methods if m in sample and sample[m]['success'])
            
            if success_count == 0:
                all_fail += 1
                all_fail_samples.append(sample['sample_id'])
            elif success_count == len(methods):
                all_success += 1
            else:
                partial_fail += 1
        
        total = len(model_data)
        print(f"  æ€»æ ·æœ¬æ•°: {total}")
        print(f"  æ‰€æœ‰æ”»å‡»éƒ½æˆåŠŸ: {all_success} ({all_success/total*100:.1f}%)")
        print(f"  éƒ¨åˆ†æ”»å‡»å¤±è´¥: {partial_fail} ({partial_fail/total*100:.1f}%)")
        print(f"  æ‰€æœ‰æ”»å‡»éƒ½å¤±è´¥: {all_fail} ({all_fail/total*100:.1f}%)")
        
        if all_fail > 0:
            print(f"  å®Œå…¨å¤±è´¥çš„æ ·æœ¬ID: {all_fail_samples[:10]}{'...' if len(all_fail_samples) > 10 else ''}")
        
        failure_stats[model] = {
            'all_success': all_success,
            'partial_fail': partial_fail,
            'all_fail': all_fail,
            'all_fail_samples': all_fail_samples
        }
    
    return failure_stats

def analyze_l0_distribution(detailed: Dict) -> None:
    """åˆ†æL0åˆ†å¸ƒ"""
    print(f"\n{'='*80}")
    print(f"ğŸ“ L0åˆ†å¸ƒåˆ†æ")
    print(f"{'='*80}\n")
    
    for model in detailed.keys():
        print(f"\nã€{model}ã€‘")
        model_data = detailed[model]
        
        for method in ['JSMA', 'One-Pixel', 'SparseFool']:
            l0_values = []
            for sample in model_data:
                if method in sample and sample[method]['success']:
                    l0_values.append(sample[method]['l0'])
            
            if len(l0_values) > 0:
                print(f"\n  {method}:")
                print(f"    æˆåŠŸæ”»å‡»æ•°: {len(l0_values)}")
                print(f"    L0å‡å€¼: {np.mean(l0_values):.2f}")
                print(f"    L0æ ‡å‡†å·®: {np.std(l0_values):.2f}")
                print(f"    L0ä¸­ä½æ•°: {np.median(l0_values):.2f}")
                print(f"    L0èŒƒå›´: [{np.min(l0_values):.0f}, {np.max(l0_values):.0f}]")
                print(f"    L0åˆ†å¸ƒ: 25%={np.percentile(l0_values, 25):.1f}, "
                      f"75%={np.percentile(l0_values, 75):.1f}")

def correlation_analysis(results: Dict) -> None:
    """ç›¸å…³æ€§åˆ†æ"""
    print(f"\n{'='*80}")
    print(f"ğŸ“ˆ ç›¸å…³æ€§åˆ†æ")
    print(f"{'='*80}\n")
    
    # å‡†ç¡®ç‡ vs å¹³å‡ASR
    models = ['ResNet18', 'VGG16', 'MobileNetV2']
    accuracies = [83.77, 92.27, 84.90]
    
    # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„å¹³å‡ASR
    avg_asrs = []
    for model in models:
        model_results = results[model]
        asrs = []
        for method in ['JSMA', 'One-Pixel', 'SparseFool']:
            if method in model_results and model_results[method]['ASR'] > 0:
                asrs.append(model_results[method]['ASR'])
        avg_asrs.append(np.mean(asrs) if asrs else 0)
    
    # è®¡ç®—ç›¸å…³ç³»æ•°
    corr, p_value = stats.pearsonr(accuracies, avg_asrs)
    
    print("æ¨¡å‹å‡†ç¡®ç‡ vs å¹³å‡ASR:")
    for model, acc, asr in zip(models, accuracies, avg_asrs):
        print(f"  {model}: å‡†ç¡®ç‡={acc:.2f}%, å¹³å‡ASR={asr:.1f}%")
    
    print(f"\n  Pearsonç›¸å…³ç³»æ•°: {corr:.3f}")
    print(f"  på€¼: {p_value:.4f}")
    
    if corr < 0 and p_value < 0.1:
        print(f"  âœ… å­˜åœ¨è´Ÿç›¸å…³ï¼šå‡†ç¡®ç‡è¶Šé«˜ï¼ŒASRè¶Šä½ï¼ˆé²æ£’æ€§è¶Šå¼ºï¼‰")
    else:
        print(f"  âš ï¸  ç›¸å…³æ€§ä¸æ˜¾è‘—")

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("ğŸ”¬ ç»Ÿè®¡åˆ†ææŠ¥å‘Š")
    print("="*80)
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“¦ åŠ è½½æ•°æ®...")
    results = load_results()
    detailed = load_detailed_results()
    print(f"âœ… åŠ è½½å®Œæˆï¼š{len(results)}ä¸ªæ¨¡å‹çš„æ•°æ®")
    
    # 1. ç›¸å…³æ€§åˆ†æ
    correlation_analysis(results)
    
    # 2. ä¸åŒæ”»å‡»æ–¹æ³•ä¹‹é—´çš„æ˜¾è‘—æ€§æ£€éªŒ
    for model in ['ResNet18', 'VGG16', 'MobileNetV2']:
        if model in detailed:
            df = significance_test_between_methods(detailed, model)
            print(f"\n{model} æ˜¾è‘—æ€§æ£€éªŒæ±‡æ€»:")
            print(df.to_string(index=False))
        else:
            print(f"\nâš ï¸  {model}: æ— è¯¦ç»†æ•°æ®ï¼Œè·³è¿‡æ–¹æ³•é—´æ£€éªŒ")
    
    # 3. ä¸åŒæ¨¡å‹ä¹‹é—´çš„æ˜¾è‘—æ€§æ£€éªŒ
    for method in ['JSMA', 'SparseFool']:  # One-Pixelæ•°æ®ä¸å®Œæ•´ï¼Œæš‚æ—¶è·³è¿‡
        df = significance_test_between_models(detailed, method)
        print(f"\n{method} æ˜¾è‘—æ€§æ£€éªŒæ±‡æ€»:")
        print(df.to_string(index=False))
    
    # 4. å¤±è´¥æ¡ˆä¾‹åˆ†æ
    failure_stats = analyze_failure_cases(detailed)
    
    # 5. L0åˆ†å¸ƒåˆ†æ
    analyze_l0_distribution(detailed)
    
    # ä¿å­˜åˆ†ææŠ¥å‘Š
    print(f"\n{'='*80}")
    print("ğŸ’¾ ä¿å­˜åˆ†ææŠ¥å‘Š...")
    
    output_dir = Path('results/statistical_analysis')
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # ä¿å­˜å¤±è´¥æ¡ˆä¾‹ç»Ÿè®¡
    with open(output_dir / 'failure_cases.json', 'w') as f:
        json.dump(failure_stats, f, indent=2)
    
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_dir}")
    
    print(f"\n{'='*80}")
    print("ğŸ‰ ç»Ÿè®¡åˆ†æå®Œæˆï¼")
    print(f"{'='*80}\n")

if __name__ == "__main__":
    main()

