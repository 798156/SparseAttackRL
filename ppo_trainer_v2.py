# ppo_trainer_v2.py
"""
ä¼˜åŒ–ç‰ˆæœ¬çš„PPOè®­ç»ƒå™¨
ä¸»è¦æ”¹è¿›ï¼š
1. CNNç­–ç•¥ç½‘ç»œï¼ˆæ›¿ä»£MLPï¼‰
2. æ”¹è¿›çš„è¶…å‚æ•°é…ç½®
3. æ›´å¥½çš„æ—¥å¿—è®°å½•
"""

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
import torch
import torch.nn as nn
import gymnasium as gym


class CNNFeatureExtractor(BaseFeaturesExtractor):
    """
    è‡ªå®šä¹‰CNNç‰¹å¾æå–å™¨
    ç”¨äºå¤„ç†å›¾åƒè¾“å…¥ï¼Œä¿ç•™ç©ºé—´ç»“æ„
    """
    
    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):
        super(CNNFeatureExtractor, self).__init__(observation_space, features_dim)
        
        # è·å–è¾“å…¥ç»´åº¦
        n_input_channels = observation_space.shape[0]  # C+3 for enhanced state
        
        # CNNæ¶æ„
        self.cnn = nn.Sequential(
            # ç¬¬ä¸€å±‚å·ç§¯: (C+3, 32, 32) -> (32, 16, 16)
            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # ç¬¬äºŒå±‚å·ç§¯: (32, 16, 16) -> (64, 8, 8)
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # ç¬¬ä¸‰å±‚å·ç§¯: (64, 8, 8) -> (64, 4, 4)
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # Flatten
            nn.Flatten(),
        )
        
        # è®¡ç®—å±•å¹³åçš„ç»´åº¦
        # å¯¹äº32x32è¾“å…¥: 64 * 4 * 4 = 1024
        with torch.no_grad():
            sample = torch.zeros(1, n_input_channels, 32, 32)
            n_flatten = self.cnn(sample).shape[1]
        
        # å…¨è¿æ¥å±‚
        self.linear = nn.Sequential(
            nn.Linear(n_flatten, features_dim),
            nn.ReLU(),
        )
        
    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        return self.linear(self.cnn(observations))


def train_rl_agent_v2(
    env, 
    timesteps=10000,
    save_path="ppo_sparse_model_v2",
    use_cnn=True,
    learning_rate=3e-4,
    verbose=1
):
    """
    ä½¿ç”¨ä¼˜åŒ–é…ç½®è®­ç»ƒ RL æ™ºèƒ½ä½“
    
    å‚æ•°:
        env: è®­ç»ƒç¯å¢ƒï¼ˆæœ€å¥½æ˜¯ SparseAttackEnvV2ï¼‰
        timesteps: è®­ç»ƒæ­¥æ•°
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        use_cnn: æ˜¯å¦ä½¿ç”¨CNNç­–ç•¥ï¼ˆæ¨èTrueï¼‰
        learning_rate: å­¦ä¹ ç‡
        verbose: æ—¥å¿—è¯¦ç»†ç¨‹åº¦
    
    è¿”å›:
        model: è®­ç»ƒå¥½çš„PPOæ¨¡å‹
    """
    vec_env = DummyVecEnv([lambda: env])
    
    if use_cnn:
        # ä½¿ç”¨CNNç­–ç•¥
        policy_kwargs = dict(
            features_extractor_class=CNNFeatureExtractor,
            features_extractor_kwargs=dict(features_dim=256),
            net_arch=[dict(pi=[128, 128], vf=[128, 128])]  # Actorå’ŒCriticçš„éšè—å±‚
        )
        
        model = PPO(
            policy="CnnPolicy",  # ä½¿ç”¨CnnPolicy
            env=vec_env,
            learning_rate=learning_rate,
            n_steps=2048,           # æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•°
            batch_size=64,          # æ‰¹æ¬¡å¤§å°
            n_epochs=10,            # æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•°
            gamma=0.99,             # æŠ˜æ‰£å› å­
            gae_lambda=0.95,        # GAEå‚æ•°
            clip_range=0.2,         # PPOè£å‰ªå‚æ•°
            ent_coef=0.01,          # ç†µç³»æ•°ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
            vf_coef=0.5,            # ä»·å€¼å‡½æ•°ç³»æ•°
            max_grad_norm=0.5,      # æ¢¯åº¦è£å‰ª
            policy_kwargs=policy_kwargs,
            verbose=verbose,
            tensorboard_log="./logs/",
            device="auto"
        )
        
        print("ğŸ§  ä½¿ç”¨ CNN ç­–ç•¥ç½‘ç»œ")
    else:
        # ä½¿ç”¨ä¼ ç»ŸMLPç­–ç•¥
        model = PPO(
            policy="MlpPolicy",
            env=vec_env,
            learning_rate=learning_rate,
            verbose=verbose,
            tensorboard_log="./logs/",
            gamma=0.99,
            ent_coef=0.01,
            batch_size=64,
            device="auto"
        )
        
        print("ğŸ§  ä½¿ç”¨ MLP ç­–ç•¥ç½‘ç»œ")
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ RL æ™ºèƒ½ä½“ï¼Œå…± {timesteps} æ­¥...")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    print(f"   ç­–ç•¥ç±»å‹: {'CNN' if use_cnn else 'MLP'}")
    
    model.learn(total_timesteps=timesteps, tb_log_name="ppo_v2_run")
    model.save(save_path)
    
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}.zip")
    
    return model


def train_with_curriculum(
    env_template,
    test_set,
    model,
    timesteps_per_stage=3000,
    save_path="ppo_curriculum"
):
    """
    è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
    ä»ç®€å•æ ·æœ¬é€æ­¥è¿‡æ¸¡åˆ°å›°éš¾æ ·æœ¬
    
    å‚æ•°:
        env_template: ç¯å¢ƒåˆ›å»ºå‡½æ•°
        test_set: æµ‹è¯•æ•°æ®é›†
        model: ç›®æ ‡æ¨¡å‹
        timesteps_per_stage: æ¯ä¸ªé˜¶æ®µçš„è®­ç»ƒæ­¥æ•°
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
    
    è¿”å›:
        agent: è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    """
    print("\n" + "=" * 60)
    print("ğŸ“š å¼€å§‹è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ")
    print("=" * 60)
    
    # æ ¹æ®ç½®ä¿¡åº¦å¯¹æ ·æœ¬è¿›è¡Œåˆ†çº§
    print("\nğŸ“Š è¯„ä¼°æ ·æœ¬éš¾åº¦...")
    difficulties = []
    
    for i in range(min(1000, len(test_set))):
        image, label = test_set[i]
        with torch.no_grad():
            output = model(image.unsqueeze(0).to(next(model.parameters()).device))
            pred = output.argmax(dim=1).item()
            conf = torch.softmax(output, dim=1)[0, label].item()
        
        if pred == label:  # åªè€ƒè™‘æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬
            difficulties.append((i, conf))
    
    # æŒ‰ç½®ä¿¡åº¦æ’åº
    difficulties.sort(key=lambda x: x[1])
    
    # åˆ†ä¸ºä¸‰ä¸ªéš¾åº¦ç­‰çº§
    n_samples = len(difficulties)
    easy_samples = [idx for idx, _ in difficulties[:n_samples//3]]
    medium_samples = [idx for idx, _ in difficulties[n_samples//3:2*n_samples//3]]
    hard_samples = [idx for idx, _ in difficulties[2*n_samples//3:]]
    
    print(f"   ç®€å•æ ·æœ¬: {len(easy_samples)} ä¸ª")
    print(f"   ä¸­ç­‰æ ·æœ¬: {len(medium_samples)} ä¸ª")
    print(f"   å›°éš¾æ ·æœ¬: {len(hard_samples)} ä¸ª")
    
    # é˜¶æ®µ1: ç®€å•æ ·æœ¬
    print("\nğŸ“– é˜¶æ®µ1: è®­ç»ƒç®€å•æ ·æœ¬ï¼ˆç½®ä¿¡åº¦ä½ï¼‰")
    sample_idx = easy_samples[0]
    image, label = test_set[sample_idx]
    env = env_template(image, label, model, max_steps=7)
    
    agent = train_rl_agent_v2(
        env, 
        timesteps=timesteps_per_stage,
        save_path=f"{save_path}_stage1",
        use_cnn=True
    )
    
    # é˜¶æ®µ2: ä¸­ç­‰æ ·æœ¬
    print("\nğŸ“– é˜¶æ®µ2: è®­ç»ƒä¸­ç­‰æ ·æœ¬")
    sample_idx = medium_samples[0]
    image, label = test_set[sample_idx]
    env = env_template(image, label, model, max_steps=5)
    
    agent.set_env(DummyVecEnv([lambda: env]))
    agent.learn(total_timesteps=timesteps_per_stage, tb_log_name="curriculum_stage2", reset_num_timesteps=False)
    agent.save(f"{save_path}_stage2")
    
    # é˜¶æ®µ3: å›°éš¾æ ·æœ¬
    print("\nğŸ“– é˜¶æ®µ3: è®­ç»ƒå›°éš¾æ ·æœ¬ï¼ˆç½®ä¿¡åº¦é«˜ï¼‰")
    sample_idx = hard_samples[0]
    image, label = test_set[sample_idx]
    env = env_template(image, label, model, max_steps=3)
    
    agent.set_env(DummyVecEnv([lambda: env]))
    agent.learn(total_timesteps=timesteps_per_stage, tb_log_name="curriculum_stage3", reset_num_timesteps=False)
    agent.save(f"{save_path}_final")
    
    print("\nâœ… è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå®Œæˆï¼")
    
    return agent


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯• PPO Trainer V2")
    
    from torchvision import datasets, transforms
    from target_model import load_target_model
    from sparse_attack_env_v2 import SparseAttackEnvV2
    
    # åŠ è½½æ•°æ®å’Œæ¨¡å‹
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    target_model = load_target_model('resnet18', num_classes=10)
    
    image, label = test_set[0]
    
    # åˆ›å»ºä¼˜åŒ–ç‰ˆç¯å¢ƒ
    env = SparseAttackEnvV2(image, label, target_model, max_steps=5, use_saliency=True)
    
    # è®­ç»ƒï¼ˆå°‘é‡æ­¥æ•°ç”¨äºæµ‹è¯•ï¼‰
    print("\nå¼€å§‹è®­ç»ƒ...")
    agent = train_rl_agent_v2(
        env, 
        timesteps=1000,  # æµ‹è¯•ç”¨ï¼Œå®é™…åº”è¯¥ç”¨10000+
        save_path="test_ppo_v2",
        use_cnn=True
    )
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")

