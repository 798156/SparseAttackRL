# diagnose_rl_training.py
"""
è¯Šæ–­RLè®­ç»ƒé—®é¢˜
æ£€æŸ¥ç¯å¢ƒã€agentã€è®­ç»ƒè¿‡ç¨‹æ˜¯å¦æ­£å¸¸
"""

import torch
import torchvision
from torchvision import transforms
from sparse_attack_env_v2 import SparseAttackEnvV2
from stable_baselines3 import PPO
import numpy as np

def main():
    print("=" * 80)
    print("ğŸ” è¯Šæ–­RLè®­ç»ƒé—®é¢˜")
    print("=" * 80)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # åŠ è½½æ¨¡å‹
    print("\n1ï¸âƒ£ åŠ è½½ResNet18æ¨¡å‹...")
    model = torchvision.models.resnet18(weights=None)
    model.fc = torch.nn.Linear(model.fc.in_features, 10)
    model.load_state_dict(torch.load('cifar10_resnet18.pth', 
                                     map_location=device, 
                                     weights_only=False))
    model = model.to(device)
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # åŠ è½½æ•°æ®
    print("\n2ï¸âƒ£ åŠ è½½æµ‹è¯•æ•°æ®...")
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    
    dataset = torchvision.datasets.CIFAR10(
        root='./data', train=False, download=True, transform=transform
    )
    
    # é€‰æ‹©ä¸€ä¸ªæ­£ç¡®åˆ†ç±»çš„æ ·æœ¬
    for i in range(100):
        image, label = dataset[i]
        with torch.no_grad():
            output = model(image.unsqueeze(0).to(device))
            pred = output.argmax(dim=1).item()
        
        if pred == label:
            test_image = image
            test_label = label
            print(f"âœ… é€‰æ‹©æ ·æœ¬{i}ï¼Œæ ‡ç­¾{label}")
            break
    
    # æµ‹è¯•ç¯å¢ƒ
    print("\n3ï¸âƒ£ æµ‹è¯•ç¯å¢ƒ...")
    env = SparseAttackEnvV2(
        clean_image=test_image,
        true_label=test_label,
        model=model,
        max_steps=5,
        use_saliency=True
    )
    
    obs, _ = env.reset()
    print(f"  è§‚æµ‹ç©ºé—´: {obs.shape}")
    print(f"  åŠ¨ä½œç©ºé—´: {env.action_space}")
    
    # æµ‹è¯•éšæœºåŠ¨ä½œ
    print("\n4ï¸âƒ£ æµ‹è¯•éšæœºåŠ¨ä½œ...")
    for step in range(5):
        action = env.action_space.sample()
        obs, reward, terminated, truncated, info = env.step(action)
        print(f"  Step {step+1}: reward={reward:.3f}, done={terminated or truncated}")
    
    # æ£€æŸ¥æœ€ç»ˆçŠ¶æ€
    with torch.no_grad():
        output = model(env.current_image)
        pred = output.argmax(dim=1).item()
        conf = torch.softmax(output, dim=1)[0, test_label].item()
    
    print(f"  åŸå§‹æ ‡ç­¾: {test_label}")
    print(f"  æœ€ç»ˆé¢„æµ‹: {pred}")
    print(f"  ç½®ä¿¡åº¦: {conf:.3f}")
    print(f"  ä¿®æ”¹æ­¥æ•°: {env.current_step}")
    
    # åŠ è½½è®­ç»ƒå¥½çš„agent
    print("\n5ï¸âƒ£ åŠ è½½è®­ç»ƒå¥½çš„RL agent...")
    try:
        agent = PPO.load('models/ppo_resnet18_v3', device=device)
        print("âœ… AgentåŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ AgentåŠ è½½å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•agent
    print("\n6ï¸âƒ£ æµ‹è¯•Agentæ”»å‡»...")
    env = SparseAttackEnvV2(
        clean_image=test_image,
        true_label=test_label,
        model=model,
        max_steps=5,
        use_saliency=True
    )
    
    obs, _ = env.reset()
    
    for step in range(10):
        action, _ = agent.predict(obs, deterministic=True)
        print(f"  Step {step+1}: action={action}")
        obs, reward, terminated, truncated, info = env.step(action)
        print(f"           reward={reward:.3f}, done={terminated or truncated}")
        
        if terminated or truncated:
            break
    
    # æ£€æŸ¥ç»“æœ
    with torch.no_grad():
        output = model(env.current_image)
        pred = output.argmax(dim=1).item()
        conf = torch.softmax(output, dim=1)[0, test_label].item()
    
    print(f"\næ”»å‡»ç»“æœ:")
    print(f"  åŸå§‹æ ‡ç­¾: {test_label}")
    print(f"  æœ€ç»ˆé¢„æµ‹: {pred}")
    print(f"  ç½®ä¿¡åº¦: {conf:.3f}")
    print(f"  ä¿®æ”¹æ­¥æ•°: {env.current_step}")
    print(f"  æˆåŠŸ: {pred != test_label}")
    
    # æ£€æŸ¥agentçš„è¾“å‡ºåˆ†å¸ƒ
    print("\n7ï¸âƒ£ æ£€æŸ¥Agentè¾“å‡ºåˆ†å¸ƒ...")
    env = SparseAttackEnvV2(
        clean_image=test_image,
        true_label=test_label,
        model=model,
        max_steps=5,
        use_saliency=True
    )
    obs, _ = env.reset()
    
    actions = []
    for _ in range(20):
        action, _ = agent.predict(obs, deterministic=False)
        actions.append(action)
    
    actions = np.array(actions)
    print(f"  åŠ¨ä½œæ ·æœ¬æ•°: {len(actions)}")
    print(f"  X ä½ç½®èŒƒå›´: {actions[:, 0].min():.1f} - {actions[:, 0].max():.1f}")
    print(f"  Y ä½ç½®èŒƒå›´: {actions[:, 1].min():.1f} - {actions[:, 1].max():.1f}")
    print(f"  R æ‰°åŠ¨èŒƒå›´: {actions[:, 2].min():.3f} - {actions[:, 2].max():.3f}")
    print(f"  G æ‰°åŠ¨èŒƒå›´: {actions[:, 3].min():.3f} - {actions[:, 3].max():.3f}")
    print(f"  B æ‰°åŠ¨èŒƒå›´: {actions[:, 4].min():.3f} - {actions[:, 4].max():.3f}")
    
    # é—®é¢˜è¯Šæ–­
    print("\n" + "=" * 80)
    print("ğŸ”§ é—®é¢˜è¯Šæ–­")
    print("=" * 80)
    
    if actions[:, 2:].max() < 0.01:
        print("âš ï¸  é—®é¢˜1: RGBæ‰°åŠ¨å¤ªå°ï¼Agentæ²¡æœ‰å­¦ä¼šå¤§å¹…ä¿®æ”¹åƒç´ ")
    
    if len(set(actions[:, 0])) < 5:
        print("âš ï¸  é—®é¢˜2: Xä½ç½®å¤šæ ·æ€§ä½ï¼Agentæ€»æ˜¯ä¿®æ”¹ç›¸åŒä½ç½®")
    
    if len(set(actions[:, 1])) < 5:
        print("âš ï¸  é—®é¢˜3: Yä½ç½®å¤šæ ·æ€§ä½ï¼Agentæ€»æ˜¯ä¿®æ”¹ç›¸åŒä½ç½®")
    
    print("\nå¯èƒ½çš„åŸå› :")
    print("1. å¥–åŠ±ä¿¡å·å¤ªå¼±ï¼Œagentæ²¡æœ‰å­¦åˆ°æœ‰æ•ˆç­–ç•¥")
    print("2. è®­ç»ƒæ ·æœ¬å¤ªéš¾ï¼Œagentæ”¾å¼ƒå­¦ä¹ ")
    print("3. max_stepså¤ªå°ï¼ˆ5æ­¥ï¼‰ï¼Œé™åˆ¶äº†agent")
    print("4. ç¯å¢ƒé…ç½®æœ‰é—®é¢˜")
    
    print("\nå»ºè®®çš„è§£å†³æ–¹æ¡ˆ:")
    print("1. å¢åŠ max_steps: 5 â†’ 10")
    print("2. è°ƒæ•´å¥–åŠ±å‡½æ•°æƒé‡")
    print("3. ä½¿ç”¨æ›´ç®€å•çš„æ ·æœ¬ï¼ˆä½ç½®ä¿¡åº¦ï¼‰")
    print("4. å¢åŠ è®­ç»ƒæ­¥æ•°: 50k â†’ 100k")


if __name__ == '__main__':
    main()








