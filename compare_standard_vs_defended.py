"""
å¯¹æ¯”æ ‡å‡†æ¨¡å‹ vs é˜²å¾¡æ¨¡å‹çš„æ”»å‡»ç»“æœ

ç”Ÿæˆï¼š
1. å®Œæ•´å¯¹æ¯”è¡¨
2. ASRä¸‹é™å¹…åº¦åˆ†æ
3. ç›¸å¯¹æ€§èƒ½ä¿æŒåº¦
4. å¯è§†åŒ–å›¾è¡¨
5. è®ºæ–‡ç”¨LaTeXè¡¨æ ¼
"""

import json
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from scipy import stats

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
sns.set_palette("husl")

def load_results(model_type):
    """åŠ è½½ç»“æœæ•°æ®"""
    if model_type == 'standard':
        base_dir = Path('results/complete_baseline')
        prefix = 'resnet18'  # æ ‡å‡†æ¨¡å‹çš„ResNet18ç»“æœ
    else:  # defended
        base_dir = Path('results/defended_model')
        prefix = 'defended'
    
    methods = ['jsma', 'sparsefool', 'greedy', 'randomsparse', 'pixelgrad']
    results = {}
    
    for method in methods:
        file_path = base_dir / f'{prefix}_{method}.json'
        if file_path.exists():
            with open(file_path, 'r') as f:
                data = json.load(f)
                results[method] = data
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°: {file_path}")
    
    return results

def calculate_summary(results):
    """è®¡ç®—æ±‡æ€»ç»Ÿè®¡"""
    summary = {}
    
    for method, data in results.items():
        # æ”¯æŒä¸¤ç§JSONæ ¼å¼
        # æ ¼å¼1: {"samples": [...]}  (test_on_defended_model.pyæ ¼å¼)
        # æ ¼å¼2: {"asr": ..., "avg_l0": ..., "detailed_results": [...]}  (final_test_with_greedy.pyæ ¼å¼)
        
        if 'asr' in data and 'avg_l0' in data:
            # æ ¼å¼2ï¼šç›´æ¥ä½¿ç”¨é¡¶å±‚ç»Ÿè®¡æ•°æ®
            summary[method] = {
                'asr': data.get('asr', 0),
                'avg_l0': data.get('avg_l0', 0),
                'avg_l2': data.get('avg_l2', 0),
                'avg_ssim': data.get('avg_ssim', 0),
                'avg_time': data.get('avg_time', 0),
                'num_samples': data.get('total_samples', 0)
            }
        else:
            # æ ¼å¼1ï¼šä»samplesè®¡ç®—
            samples = data.get('samples', [])
            successes = [s for s in samples if s.get('success', False)]
            
            asr = len(successes) / len(samples) * 100 if samples else 0
            
            if successes:
                avg_l0 = np.mean([s.get('l0', 0) for s in successes])
                avg_l2 = np.mean([s.get('l2', 0) for s in successes])
                avg_ssim = np.mean([s.get('ssim', 0) for s in successes])
            else:
                avg_l0 = avg_l2 = avg_ssim = 0
            
            avg_time = np.mean([s.get('time', 0) for s in samples]) if samples else 0
            
            summary[method] = {
                'asr': asr,
                'avg_l0': avg_l0,
                'avg_l2': avg_l2,
                'avg_ssim': avg_ssim,
                'avg_time': avg_time,
                'num_samples': len(samples)
            }
    
    return summary

def compare_results(standard_summary, defended_summary):
    """å¯¹æ¯”åˆ†æ"""
    comparison = {}
    
    for method in standard_summary.keys():
        if method in defended_summary:
            std = standard_summary[method]
            def_ = defended_summary[method]
            
            asr_drop = std['asr'] - def_['asr']
            asr_drop_pct = (asr_drop / std['asr'] * 100) if std['asr'] > 0 else 0
            
            comparison[method] = {
                'standard_asr': std['asr'],
                'defended_asr': def_['asr'],
                'asr_drop': asr_drop,
                'asr_drop_pct': asr_drop_pct,
                'standard_l0': std['avg_l0'],
                'defended_l0': def_['avg_l0'],
            }
    
    return comparison

def print_comparison_table(standard_summary, defended_summary, comparison):
    """æ‰“å°å¯¹æ¯”è¡¨"""
    print("\n" + "="*100)
    print("ğŸ“Š Standard vs Defended Model - Complete Comparison")
    print("="*100)
    
    print(f"\n{'Method':<15} {'Standard ASR':<15} {'Defended ASR':<15} {'Drop':<12} {'Drop %':<10}")
    print("-"*100)
    
    method_names = {
        'jsma': 'JSMA',
        'sparsefool': 'SparseFool',
        'greedy': 'Greedy',
        'pixelgrad': 'PixelGrad',
        'randomsparse': 'RandomSparse'
    }
    
    for method in ['jsma', 'greedy', 'sparsefool', 'pixelgrad', 'randomsparse']:
        if method in comparison:
            comp = comparison[method]
            name = method_names.get(method, method)
            print(f"{name:<15} "
                  f"{comp['standard_asr']:>12.1f}%   "
                  f"{comp['defended_asr']:>12.1f}%   "
                  f"{comp['asr_drop']:>9.1f}%  "
                  f"{comp['asr_drop_pct']:>8.1f}%")
    
    print("\n" + "="*100)
    print("ğŸ“Š Detailed Metrics Comparison")
    print("="*100)
    
    print(f"\n{'Method':<15} {'Model':<12} {'ASR':<10} {'L0':<8} {'L2':<10} {'SSIM':<8} {'Time(s)':<8}")
    print("-"*100)
    
    for method in ['jsma', 'greedy', 'sparsefool', 'pixelgrad', 'randomsparse']:
        if method in standard_summary:
            name = method_names.get(method, method)
            std = standard_summary[method]
            def_ = defended_summary.get(method, {})
            
            print(f"{name:<15} {'Standard':<12} {std['asr']:>7.1f}% "
                  f"{std['avg_l0']:>6.2f}  {std['avg_l2']:>8.4f}  "
                  f"{std['avg_ssim']:>6.4f}  {std['avg_time']:>6.3f}")
            
            if def_:
                print(f"{'':<15} {'Defended':<12} {def_['asr']:>7.1f}% "
                      f"{def_['avg_l0']:>6.2f}  {def_['avg_l2']:>8.4f}  "
                      f"{def_['avg_ssim']:>6.4f}  {def_['avg_time']:>6.3f}")
                print()

def analyze_ranking_consistency(standard_summary, defended_summary):
    """åˆ†ææ’åä¸€è‡´æ€§"""
    print("\n" + "="*100)
    print("ğŸ“ˆ Ranking Consistency Analysis")
    print("="*100)
    
    # æŒ‰ASRæ’åº
    std_ranking = sorted(standard_summary.items(), key=lambda x: x[1]['asr'], reverse=True)
    def_ranking = sorted(defended_summary.items(), key=lambda x: x[1]['asr'], reverse=True)
    
    print("\nğŸ† ASR Ranking:")
    print("\nStandard Model:")
    for i, (method, summary) in enumerate(std_ranking, 1):
        print(f"  {i}. {method.upper():<15} {summary['asr']:.1f}%")
    
    print("\nDefended Model:")
    for i, (method, summary) in enumerate(def_ranking, 1):
        print(f"  {i}. {method.upper():<15} {summary['asr']:.1f}%")
    
    # è®¡ç®—Spearmanç›¸å…³ç³»æ•°
    std_ranks = {m: i for i, (m, _) in enumerate(std_ranking)}
    def_ranks = {m: i for i, (m, _) in enumerate(def_ranking)}
    
    common_methods = set(std_ranks.keys()) & set(def_ranks.keys())
    std_rank_values = [std_ranks[m] for m in sorted(common_methods)]
    def_rank_values = [def_ranks[m] for m in sorted(common_methods)]
    
    if len(std_rank_values) > 1:
        correlation, p_value = stats.spearmanr(std_rank_values, def_rank_values)
        print(f"\nğŸ“Š Spearman Rank Correlation: {correlation:.3f} (p={p_value:.4f})")
        
        if correlation > 0.8:
            print("   âœ… Very strong correlation - ranking highly consistent!")
        elif correlation > 0.6:
            print("   âœ… Strong correlation - ranking mostly consistent")
        else:
            print("   âš ï¸ Moderate correlation - some ranking changes")

def plot_asr_comparison(comparison, output_dir):
    """ç»˜åˆ¶ASRå¯¹æ¯”å›¾"""
    methods = list(comparison.keys())
    method_labels = {
        'jsma': 'JSMA',
        'sparsefool': 'SparseFool', 
        'greedy': 'Greedy',
        'pixelgrad': 'PixelGrad',
        'randomsparse': 'RandomSparse'
    }
    
    labels = [method_labels.get(m, m) for m in methods]
    standard_asrs = [comparison[m]['standard_asr'] for m in methods]
    defended_asrs = [comparison[m]['defended_asr'] for m in methods]
    
    x = np.arange(len(labels))
    width = 0.35
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bars1 = ax.bar(x - width/2, standard_asrs, width, label='Standard Model', alpha=0.8)
    bars2 = ax.bar(x + width/2, defended_asrs, width, label='Defended Model', alpha=0.8)
    
    ax.set_xlabel('Attack Method', fontsize=12)
    ax.set_ylabel('Attack Success Rate (%)', fontsize=12)
    ax.set_title('ASR Comparison: Standard vs Defended Model', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=0)
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.1f}%', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'asr_standard_vs_defended.png', dpi=300, bbox_inches='tight')
    plt.savefig(output_dir / 'asr_standard_vs_defended.pdf', bbox_inches='tight')
    plt.close()
    
    print(f"  âœ… ä¿å­˜: asr_standard_vs_defended.png/pdf")

def plot_asr_drop(comparison, output_dir):
    """ç»˜åˆ¶ASRä¸‹é™å¹…åº¦å›¾"""
    methods = list(comparison.keys())
    method_labels = {
        'jsma': 'JSMA',
        'sparsefool': 'SparseFool',
        'greedy': 'Greedy',
        'pixelgrad': 'PixelGrad',
        'randomsparse': 'RandomSparse'
    }
    
    labels = [method_labels.get(m, m) for m in methods]
    drops = [comparison[m]['asr_drop'] for m in methods]
    drop_pcts = [comparison[m]['asr_drop_pct'] for m in methods]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # ç»å¯¹ä¸‹é™
    bars1 = ax1.bar(labels, drops, alpha=0.8, color='coral')
    ax1.set_ylabel('ASR Drop (Percentage Points)', fontsize=12)
    ax1.set_title('Absolute ASR Drop on Defended Model', fontsize=12, fontweight='bold')
    ax1.grid(axis='y', alpha=0.3)
    
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)
    
    # ç›¸å¯¹ä¸‹é™
    bars2 = ax2.bar(labels, drop_pcts, alpha=0.8, color='steelblue')
    ax2.set_ylabel('Relative ASR Drop (%)', fontsize=12)
    ax2.set_title('Relative ASR Drop on Defended Model', fontsize=12, fontweight='bold')
    ax2.grid(axis='y', alpha=0.3)
    
    for bar in bars2:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'asr_drop_analysis.png', dpi=300, bbox_inches='tight')
    plt.savefig(output_dir / 'asr_drop_analysis.pdf', bbox_inches='tight')
    plt.close()
    
    print(f"  âœ… ä¿å­˜: asr_drop_analysis.png/pdf")

def generate_latex_table(standard_summary, defended_summary, comparison, output_dir):
    """ç”ŸæˆLaTeXè¡¨æ ¼"""
    latex_code = r"""
\begin{table}[htbp]
\centering
\caption{Comparison of Attack Methods on Standard vs Defended Models}
\label{tab:standard_vs_defended}
\begin{tabular}{l|cc|cc}
\hline
\textbf{Method} & \multicolumn{2}{c|}{\textbf{ASR (\%)}} & \multicolumn{2}{c}{\textbf{Avg L0}} \\
& Standard & Defended & Standard & Defended \\
\hline
"""
    
    method_order = ['jsma', 'greedy', 'sparsefool', 'pixelgrad', 'randomsparse']
    method_names = {
        'jsma': 'JSMA',
        'sparsefool': 'SparseFool',
        'greedy': 'Greedy',
        'pixelgrad': 'PixelGrad',
        'randomsparse': 'RandomSparse'
    }
    
    for method in method_order:
        if method in comparison:
            comp = comparison[method]
            name = method_names.get(method, method)
            latex_code += f"{name} & "
            latex_code += f"{comp['standard_asr']:.1f} & {comp['defended_asr']:.1f} & "
            latex_code += f"{comp['standard_l0']:.2f} & {comp['defended_l0']:.2f} \\\\\n"
    
    latex_code += r"""\hline
\end{tabular}
\end{table}
"""
    
    output_file = output_dir / 'latex_table_standard_vs_defended.tex'
    with open(output_file, 'w') as f:
        f.write(latex_code)
    
    print(f"  âœ… ä¿å­˜: latex_table_standard_vs_defended.tex")

def write_analysis_report(standard_summary, defended_summary, comparison, output_dir):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    report = """# æ ‡å‡†æ¨¡å‹ vs é˜²å¾¡æ¨¡å‹ - å®Œæ•´å¯¹æ¯”åˆ†æ

## 1. å®éªŒæ¦‚è¿°

æœ¬åˆ†æå¯¹æ¯”äº†5ç§ç¨€ç–å¯¹æŠ—æ”»å‡»æ–¹æ³•åœ¨æ ‡å‡†æ¨¡å‹å’Œé˜²å¾¡æ¨¡å‹ä¸Šçš„æ€§èƒ½ã€‚

**æµ‹è¯•é…ç½®ï¼š**
- æ ‡å‡†æ¨¡å‹ï¼šResNet18 (CIFAR-10, ~88% accuracy)
- é˜²å¾¡æ¨¡å‹ï¼šRobustBenchå¯¹æŠ—è®­ç»ƒResNet18 (~83-85% accuracy)
- æ”»å‡»æ–¹æ³•ï¼šJSMA, SparseFool, Greedy, PixelGrad, RandomSparse
- æµ‹è¯•æ ·æœ¬ï¼šæ¯ä¸ªé…ç½®100ä¸ªæ ·æœ¬

---

## 2. ä¸»è¦å‘ç°

"""
    
    # è®¡ç®—å¹³å‡ä¸‹é™
    avg_drop = np.mean([comp['asr_drop'] for comp in comparison.values()])
    avg_drop_pct = np.mean([comp['asr_drop_pct'] for comp in comparison.values()])
    
    report += f"""
### å‘ç°1ï¼šé˜²å¾¡æ¨¡å‹æ˜¾è‘—é™ä½äº†æ”»å‡»æˆåŠŸç‡

- **å¹³å‡ASRä¸‹é™ï¼š** {avg_drop:.1f} ä¸ªç™¾åˆ†ç‚¹
- **å¹³å‡ç›¸å¯¹ä¸‹é™ï¼š** {avg_drop_pct:.1f}%

è¿™è¯æ˜äº†å¯¹æŠ—è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚

"""
    
    # æ‰¾å‡ºä¸‹é™æœ€å¤šå’Œæœ€å°‘çš„æ–¹æ³•
    max_drop_method = max(comparison.items(), key=lambda x: x[1]['asr_drop_pct'])
    min_drop_method = min(comparison.items(), key=lambda x: x[1]['asr_drop_pct'])
    
    report += f"""
### å‘ç°2ï¼šä¸åŒæ–¹æ³•å¯¹é˜²å¾¡çš„æ•æ„Ÿåº¦ä¸åŒ

- **æœ€æ•æ„Ÿæ–¹æ³•ï¼š** {max_drop_method[0].upper()} (ä¸‹é™ {max_drop_method[1]['asr_drop_pct']:.1f}%)
- **æœ€ç¨³å®šæ–¹æ³•ï¼š** {min_drop_method[0].upper()} (ä¸‹é™ {min_drop_method[1]['asr_drop_pct']:.1f}%)

**è§£é‡Šï¼š**
- æŸäº›æ–¹æ³•æ›´ä¾èµ–äºæ¨¡å‹çš„è„†å¼±æ€§ï¼Œåœ¨é˜²å¾¡æ¨¡å‹ä¸Šæ€§èƒ½ä¸‹é™æ˜æ˜¾
- æŸäº›æ–¹æ³•å…·æœ‰æ›´å¥½çš„é²æ£’æ€§ï¼Œåœ¨é˜²å¾¡åœºæ™¯ä¸‹ç›¸å¯¹ç¨³å®š

"""
    
    # æ’åä¸€è‡´æ€§
    std_ranking = sorted(standard_summary.items(), key=lambda x: x[1]['asr'], reverse=True)
    def_ranking = sorted(defended_summary.items(), key=lambda x: x[1]['asr'], reverse=True)
    
    std_top3 = [m for m, _ in std_ranking[:3]]
    def_top3 = [m for m, _ in def_ranking[:3]]
    
    common_top3 = set(std_top3) & set(def_top3)
    
    report += f"""
### å‘ç°3ï¼šæ–¹æ³•ç›¸å¯¹æ’å{"åŸºæœ¬ä¿æŒ" if len(common_top3) >= 2 else "æœ‰æ‰€å˜åŒ–"}

**æ ‡å‡†æ¨¡å‹Top 3ï¼š** {', '.join([m.upper() for m in std_top3])}
**é˜²å¾¡æ¨¡å‹Top 3ï¼š** {', '.join([m.upper() for m in def_top3])}

{"âœ… å‰3åä¸­æœ‰" + str(len(common_top3)) + "ä¸ªæ–¹æ³•ä¿æŒï¼Œè¯´æ˜æ–¹æ³•çš„ç›¸å¯¹æ€§èƒ½åœ¨é˜²å¾¡åœºæ™¯ä¸‹ç¨³å®šã€‚" if len(common_top3) >= 2 else "âš ï¸ æ’åæœ‰æ˜æ˜¾å˜åŒ–ï¼Œä¸åŒæ–¹æ³•å¯¹é˜²å¾¡çš„é€‚åº”æ€§ä¸åŒã€‚"}

"""
    
    # RandomSparseåˆ†æ
    if 'randomsparse' in comparison:
        rs = comparison['randomsparse']
        report += f"""
### å‘ç°4ï¼šRandomSparseä»ç„¶æ˜¯æœ€å·®çš„baseline

- **æ ‡å‡†æ¨¡å‹ASRï¼š** {rs['standard_asr']:.1f}%
- **é˜²å¾¡æ¨¡å‹ASRï¼š** {rs['defended_asr']:.1f}%
- **ä¸‹é™ï¼š** {rs['asr_drop']:.1f} ç™¾åˆ†ç‚¹

å³ä½¿åœ¨é˜²å¾¡æ¨¡å‹ä¸Šï¼ŒRandomSparseçš„ASRä»ç„¶æ˜¾è‘—ä½äºæ‰€æœ‰æ™ºèƒ½æ–¹æ³•ï¼Œ
å†æ¬¡è¯æ˜äº†æ¢¯åº¦å¼•å¯¼çš„åƒç´ é€‰æ‹©ç­–ç•¥çš„é‡è¦æ€§ã€‚

"""
    
    report += """
---

## 3. è®ºæ–‡å†™ä½œå»ºè®®

### 3.1 å®éªŒç« èŠ‚

```latex
We further evaluate all methods on adversarially trained models 
from RobustBench to assess their practical applicability in 
defended scenarios. As expected, all methods show reduced ASR 
on the defended model, with an average drop of XX%. However, 
the relative performance ranking remains largely consistent, 
demonstrating the robustness of our findings.
```

### 3.2 è®¨è®ºç« èŠ‚

å¯ä»¥è®¨è®ºï¼š
1. ä¸åŒæ–¹æ³•å¯¹é˜²å¾¡çš„æ•æ„Ÿåº¦å·®å¼‚
2. ä¸ºä»€ä¹ˆæŸäº›æ–¹æ³•æ›´é²æ£’ï¼Ÿ
3. è¿™å¯¹å®é™…éƒ¨ç½²æœ‰ä»€ä¹ˆå¯ç¤ºï¼Ÿ

### 3.3 å¯èƒ½çš„é¢å¤–è´¡çŒ®

å¦‚æœå‘ç°äº†æœ‰è¶£çš„æ¨¡å¼ï¼ˆä¾‹å¦‚æŸä¸ªæ–¹æ³•ç‰¹åˆ«ç¨³å®šï¼‰ï¼Œå¯ä»¥ï¼š
- ä¸“é—¨åˆ†æåŸå› 
- ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„å‘ç°
- å¢å¼ºè®ºæ–‡çš„æ·±åº¦

---

## 4. æ•°æ®è¡¨æ ¼

è¯¦è§ç”Ÿæˆçš„LaTeXè¡¨æ ¼å’Œå›¾è¡¨ã€‚

---

## 5. ä¸‹ä¸€æ­¥å»ºè®®

1. âœ… æ£€æŸ¥æ‰€æœ‰æ•°æ®çš„åˆç†æ€§
2. âœ… ç¡®è®¤å‘ç°æ˜¯å¦æœ‰ä»·å€¼
3. âœ… å‡†å¤‡è®ºæ–‡å›¾è¡¨
4. ğŸ¯ ç»§ç»­Week 1 Day 5ï¼šæ•°æ®æ•´ç†
5. ğŸ¯ å¼€å§‹Week 2ï¼šè¡¥å……åˆ†æ

---

*ç”Ÿæˆæ—¶é—´ï¼šè‡ªåŠ¨ç”Ÿæˆ*
"""
    
    output_file = output_dir / 'analysis_standard_vs_defended.md'
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"  âœ… ä¿å­˜: analysis_standard_vs_defended.md")

def main():
    """ä¸»æµç¨‹"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              ğŸ“Š Standard vs Defended Model Comparison                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    standard_results = load_results('standard')
    defended_results = load_results('defended')
    
    if not standard_results or not defended_results:
        print("\nâŒ é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„ç»“æœæ–‡ä»¶")
        print("è¯·ç¡®ä¿å·²è¿è¡Œï¼š")
        print("  1. python final_test_with_greedy.py")
        print("  2. python test_new_2methods.py")  
        print("  3. python test_on_defended_model.py")
        return
    
    # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
    print("ğŸ“Š è®¡ç®—ç»Ÿè®¡...")
    standard_summary = calculate_summary(standard_results)
    defended_summary = calculate_summary(defended_results)
    
    # å¯¹æ¯”åˆ†æ
    comparison = compare_results(standard_summary, defended_summary)
    
    # æ‰“å°å¯¹æ¯”è¡¨
    print_comparison_table(standard_summary, defended_summary, comparison)
    
    # æ’åä¸€è‡´æ€§åˆ†æ
    analyze_ranking_consistency(standard_summary, defended_summary)
    
    # ç”Ÿæˆå›¾è¡¨
    output_dir = Path('results/paper_materials')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plot_asr_comparison(comparison, output_dir)
    plot_asr_drop(comparison, output_dir)
    
    # ç”ŸæˆLaTeXè¡¨æ ¼
    print(f"\nğŸ“ ç”ŸæˆLaTeXè¡¨æ ¼...")
    generate_latex_table(standard_summary, defended_summary, comparison, output_dir)
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    print(f"\nğŸ“„ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    write_analysis_report(standard_summary, defended_summary, comparison, output_dir)
    
    print(f"\n{'='*100}")
    print("ğŸ‰ å¯¹æ¯”åˆ†æå®Œæˆï¼")
    print(f"{'='*100}")
    print(f"\nğŸ’¾ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("\nğŸ“‚ ç”Ÿæˆçš„æ–‡ä»¶ï¼š")
    print("  - asr_standard_vs_defended.png/pdf")
    print("  - asr_drop_analysis.png/pdf")
    print("  - latex_table_standard_vs_defended.tex")
    print("  - analysis_standard_vs_defended.md")
    
    print(f"\n{'='*100}")
    print("ğŸ“ˆ ä¸‹ä¸€æ­¥ï¼š")
    print(f"{'='*100}")
    print("1. æŸ¥çœ‹åˆ†ææŠ¥å‘Šäº†è§£è¯¦ç»†å‘ç°")
    print("2. æ£€æŸ¥å›¾è¡¨è´¨é‡")
    print("3. å‡†å¤‡Week 1 Day 5æ•°æ®æ•´ç†")
    print("4. å¼€å§‹Week 2è¡¥å……åˆ†æ")

if __name__ == "__main__":
    main()

