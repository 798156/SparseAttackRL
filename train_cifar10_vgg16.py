# train_cifar10_vgg16.py
"""
è®­ç»ƒCIFAR-10 VGG16æ¨¡å‹
ä½¿ç”¨é¢„è®­ç»ƒæƒé‡å¾®è°ƒ + æ•°æ®å¢å¼º

é¢„è®¡æ—¶é—´ï¼šGPUçº¦30-40åˆ†é’Ÿ
ç›®æ ‡å‡†ç¡®ç‡ï¼š80-85%
"""

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import os


def main():
    print("=" * 80)
    print("ğŸš€ è®­ç»ƒCIFAR-10 VGG16 - é¢„è®­ç»ƒå¾®è°ƒæ–¹å¼")
    print("=" * 80)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    if device == 'cpu':
        print("âš ï¸  è­¦å‘Šï¼šä½¿ç”¨CPUè®­ç»ƒä¼šéå¸¸æ…¢ï¼å»ºè®®ä½¿ç”¨GPU")
        response = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
        if response.lower() != 'y':
            return

    # æ•°æ®å¢å¼º
    print("\nğŸ“¦ åŠ è½½CIFAR-10æ•°æ®...")
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    num_workers = 0
    
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=num_workers)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=num_workers)

    # ä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡
    print("\nğŸ”§ åˆ›å»ºVGG16æ¨¡å‹ï¼ˆä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡ï¼‰...")
    model = torchvision.models.vgg16(weights='IMAGENET1K_V1')
    
    # ä¿®æ”¹æœ€åçš„åˆ†ç±»å±‚
    num_ftrs = model.classifier[6].in_features
    model.classifier[6] = nn.Linear(num_ftrs, 10)
    model = model.to(device)

    # è®­ç»ƒè®¾ç½®
    criterion = nn.CrossEntropyLoss()
    
    # ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ï¼šé¢„è®­ç»ƒå±‚ç”¨å°å­¦ä¹ ç‡ï¼Œæ–°å±‚ç”¨å¤§å­¦ä¹ ç‡
    params_pretrained = []
    params_new = []
    
    for name, param in model.named_parameters():
        if 'classifier.6' in name:  # æœ€åä¸€å±‚
            params_new.append(param)
        else:  # é¢„è®­ç»ƒå±‚
            params_pretrained.append(param)
    
    optimizer = torch.optim.SGD([
        {'params': params_pretrained, 'lr': 0.01},  # é¢„è®­ç»ƒå±‚ç”¨å°å­¦ä¹ ç‡
        {'params': params_new, 'lr': 0.1}  # æ–°å±‚ç”¨å¤§å­¦ä¹ ç‡
    ], momentum=0.9, weight_decay=5e-4)
    
    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 25], gamma=0.1)

    num_epochs = 30  # VGG16éœ€è¦æ›´å¤šepoch
    print(f"\nğŸ“ å¼€å§‹è®­ç»ƒï¼ˆ{num_epochs} epochsï¼‰...")
    print(f"é¢„è®¡æ—¶é—´: GPUçº¦30-40åˆ†é’Ÿ\n")

    best_acc = 0
    save_interval = 10  # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹

    for epoch in range(num_epochs):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (inputs, targets) in enumerate(trainloader):
            inputs, targets = inputs.to(device), targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
        
        train_acc = 100. * correct / total
        
        # æµ‹è¯•
        model.eval()
        test_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, targets in testloader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                
                test_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        test_acc = 100. * correct / total
        
        current_lr = optimizer.param_groups[0]['lr']
        print(f'Epoch {epoch+1:3d}/{num_epochs} | LR: {current_lr:.4f} | '
              f'Train: {train_acc:6.2f}% | Test: {test_acc:6.2f}%', end='')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_acc > best_acc:
            print(f' ğŸ’¾ [æœ€ä½³]')
            best_acc = test_acc
            torch.save(model.state_dict(), 'cifar10_vgg16.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'accuracy': test_acc,
            }, 'cifar10_vgg16_best.pth')
        else:
            print()
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % save_interval == 0:
            checkpoint_path = f'checkpoint_vgg16_epoch_{epoch+1}.pth'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'accuracy': test_acc,
            }, checkpoint_path)
            print(f'  ğŸ’¾ æ£€æŸ¥ç‚¹ä¿å­˜: {checkpoint_path}')
        
        scheduler.step()

    print("\n" + "=" * 80)
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: cifar10_vgg16.pth")
    print("=" * 80)


if __name__ == '__main__':
    main()








